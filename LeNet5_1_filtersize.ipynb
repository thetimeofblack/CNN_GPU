{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import some keras and basic module \n",
    "\n",
    "from __future__ import print_function \n",
    "import tensorflow.keras as keras\n",
    "import tensorflow as tf\n",
    "# data set cifar 10 \n",
    "from tensorflow.keras.datasets import cifar10 \n",
    "\n",
    "# basic preprocesssing for image data \n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# construct neural network \n",
    "from tensorflow.keras.models import Sequential \n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten \n",
    "\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "\n",
    "import os \n",
    "\n",
    "import numpy as np \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## basic parameters \n",
    "batch_size = 32 \n",
    "num_classes = 10 \n",
    "num_epochs = 300 \n",
    "is_data_augmentation = False \n",
    "num_predictions = 20 \n",
    "model_dir = 'models'\n",
    "model_filename = 'keras_cifar10_model_5_filtersize.h1'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the shape of training data set is:  (50000, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "#read data and split data into train and test set\n",
    "\n",
    "(train_set,train_label),(test_set,test_label) = cifar10.load_data() \n",
    "\n",
    "print('the shape of training data set is: ',train_set.shape) \n",
    "\n",
    "# print number of train and test samples \n",
    "\n",
    "# train samples 50000\n",
    "#print(train_set.shape[0] , 'train samples') \n",
    "\n",
    "# test samples 10000\n",
    "#print(test_set.shape[0] , 'test samples') \n",
    "\n",
    "#print(train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 10)\n",
      "(10000, 10)\n"
     ]
    }
   ],
   "source": [
    "# Convert class label to binary vector \n",
    "train_label = keras.utils.to_categorical(train_label,num_classes)\n",
    "test_label  = keras.utils.to_categorical(test_label,num_classes) \n",
    "print(train_label.shape)\n",
    "print(test_label.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the code input for Conv2D \n",
    "```\n",
    "tf.keras.layers.Conv2D(\n",
    "    filters, kernel_size, strides=(1, 1), padding='valid', data_format=None,\n",
    "    dilation_rate=(1, 1), activation=None, use_bias=True,\n",
    "    kernel_initializer='glorot_uniform', bias_initializer='zeros',\n",
    "    kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None,\n",
    "    kernel_constraint=None, bias_constraint=None, **kwargs\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct the model \n",
    "\n",
    "cnn_model = Sequential() \n",
    "\n",
    "\n",
    "# convolutional layer stride 1 no padding nfilters = 6 input_shape = 32*32*3\n",
    "# acitvation = softmax\n",
    "cnn_model.add(Conv2D(20,(5,5),padding='valid',input_shape=train_set.shape[1:], ))\n",
    "cnn_model.add(Activation('relu'))\n",
    "\n",
    "# max-pooling layer window size 2*2\n",
    "cnn_model.add(MaxPooling2D(pool_size = (2,2)))\n",
    "\n",
    "# convolutional layer stride 1 no padding nfilters = 6 input_shape = 14*14*6\n",
    "# activation = softmax\n",
    "cnn_model.add(Conv2D(40,(5,5),padding ='valid'))\n",
    "cnn_model.add(Activation('relu'))\n",
    "\n",
    "# max-pooling layer window size 2*2\n",
    "cnn_model.add(MaxPooling2D(pool_size =(2,2) ))\n",
    "\n",
    "# flatten 2d to 1d \n",
    "cnn_model.add(Flatten())\n",
    "\n",
    "\n",
    "# full connected layer nfilters = 120 \n",
    "\n",
    "cnn_model.add(Dense(150,activation = 'relu' )) \n",
    "\n",
    "# full connected layer nfilters = 84 \n",
    "cnn_model.add(Dense(90, activation = 'relu'))\n",
    "\n",
    "# last full connected layer nfilters = 10 \n",
    "cnn_model.add(Dense(10 , activation = 'softmax'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not using data augmentation.\n",
      "Epoch 1/300\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x00000222E0C29A68> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Bad argument number for Name: 4, expecting 3\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x00000222E0C29A68> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Bad argument number for Name: 4, expecting 3\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "1561/1563 [============================>.] - ETA: 0s - loss: 1.9262 - accuracy: 0.3082WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x00000222DE8ED8B8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Bad argument number for Name: 4, expecting 3\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x00000222DE8ED8B8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Bad argument number for Name: 4, expecting 3\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "1563/1563 [==============================] - 18s 12ms/step - loss: 1.9258 - accuracy: 0.3083 - val_loss: 1.7271 - val_accuracy: 0.3899\n",
      "Epoch 2/300\n",
      "1563/1563 [==============================] - 18s 12ms/step - loss: 1.6527 - accuracy: 0.4074 - val_loss: 1.6202 - val_accuracy: 0.4261\n",
      "Epoch 3/300\n",
      "1563/1563 [==============================] - 19s 12ms/step - loss: 1.5312 - accuracy: 0.4494 - val_loss: 1.4750 - val_accuracy: 0.4664\n",
      "Epoch 4/300\n",
      "1563/1563 [==============================] - 20s 13ms/step - loss: 1.4543 - accuracy: 0.4764 - val_loss: 1.4283 - val_accuracy: 0.4830\n",
      "Epoch 5/300\n",
      "1563/1563 [==============================] - 20s 13ms/step - loss: 1.3981 - accuracy: 0.5007 - val_loss: 1.3732 - val_accuracy: 0.5085\n",
      "Epoch 6/300\n",
      "1563/1563 [==============================] - 21s 13ms/step - loss: 1.3497 - accuracy: 0.5192 - val_loss: 1.3549 - val_accuracy: 0.5142\n",
      "Epoch 7/300\n",
      "1563/1563 [==============================] - 20s 13ms/step - loss: 1.3086 - accuracy: 0.5359 - val_loss: 1.3105 - val_accuracy: 0.5320\n",
      "Epoch 8/300\n",
      "1563/1563 [==============================] - 21s 14ms/step - loss: 1.2722 - accuracy: 0.5488 - val_loss: 1.2700 - val_accuracy: 0.5515\n",
      "Epoch 9/300\n",
      "1563/1563 [==============================] - 22s 14ms/step - loss: 1.2405 - accuracy: 0.5622 - val_loss: 1.2592 - val_accuracy: 0.5536\n",
      "Epoch 10/300\n",
      "1563/1563 [==============================] - 22s 14ms/step - loss: 1.2131 - accuracy: 0.5709 - val_loss: 1.2692 - val_accuracy: 0.5461\n",
      "Epoch 11/300\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 1.1893 - accuracy: 0.5802 - val_loss: 1.2107 - val_accuracy: 0.5705\n",
      "Epoch 12/300\n",
      "1563/1563 [==============================] - 26s 17ms/step - loss: 1.1657 - accuracy: 0.5882 - val_loss: 1.2054 - val_accuracy: 0.5757\n",
      "Epoch 13/300\n",
      "1563/1563 [==============================] - 24s 15ms/step - loss: 1.1447 - accuracy: 0.5944 - val_loss: 1.1750 - val_accuracy: 0.5830\n",
      "Epoch 14/300\n",
      "1563/1563 [==============================] - 24s 15ms/step - loss: 1.1266 - accuracy: 0.6044 - val_loss: 1.2065 - val_accuracy: 0.5756\n",
      "Epoch 15/300\n",
      "1563/1563 [==============================] - 26s 17ms/step - loss: 1.1075 - accuracy: 0.6110 - val_loss: 1.2209 - val_accuracy: 0.5721\n",
      "Epoch 16/300\n",
      "1563/1563 [==============================] - 26s 17ms/step - loss: 1.0903 - accuracy: 0.6158 - val_loss: 1.1527 - val_accuracy: 0.5944\n",
      "Epoch 17/300\n",
      "1563/1563 [==============================] - 26s 17ms/step - loss: 1.0743 - accuracy: 0.6211 - val_loss: 1.1751 - val_accuracy: 0.5849\n",
      "Epoch 18/300\n",
      "1563/1563 [==============================] - 26s 17ms/step - loss: 1.0592 - accuracy: 0.6276 - val_loss: 1.1296 - val_accuracy: 0.6051\n",
      "Epoch 19/300\n",
      "1563/1563 [==============================] - 26s 17ms/step - loss: 1.0440 - accuracy: 0.6323 - val_loss: 1.1209 - val_accuracy: 0.6097\n",
      "Epoch 20/300\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 1.0298 - accuracy: 0.6385 - val_loss: 1.1119 - val_accuracy: 0.6121\n",
      "Epoch 21/300\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 1.0162 - accuracy: 0.6425 - val_loss: 1.1846 - val_accuracy: 0.5782\n",
      "Epoch 22/300\n",
      "1563/1563 [==============================] - 24s 16ms/step - loss: 1.0016 - accuracy: 0.6494 - val_loss: 1.1018 - val_accuracy: 0.6133\n",
      "Epoch 23/300\n",
      "1563/1563 [==============================] - 24s 15ms/step - loss: 0.9891 - accuracy: 0.6542 - val_loss: 1.1369 - val_accuracy: 0.6040\n",
      "Epoch 24/300\n",
      "1563/1563 [==============================] - 24s 15ms/step - loss: 0.9765 - accuracy: 0.6579 - val_loss: 1.0667 - val_accuracy: 0.6324\n",
      "Epoch 25/300\n",
      "1563/1563 [==============================] - 24s 15ms/step - loss: 0.9649 - accuracy: 0.6629 - val_loss: 1.0634 - val_accuracy: 0.6308\n",
      "Epoch 26/300\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 0.9523 - accuracy: 0.6677 - val_loss: 1.0742 - val_accuracy: 0.6267\n",
      "Epoch 27/300\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 0.9410 - accuracy: 0.6711 - val_loss: 1.0718 - val_accuracy: 0.6345\n",
      "Epoch 28/300\n",
      "1563/1563 [==============================] - 24s 15ms/step - loss: 0.9299 - accuracy: 0.6731 - val_loss: 1.0762 - val_accuracy: 0.6256\n",
      "Epoch 29/300\n",
      "1563/1563 [==============================] - 24s 15ms/step - loss: 0.9189 - accuracy: 0.6799 - val_loss: 1.1003 - val_accuracy: 0.6259\n",
      "Epoch 30/300\n",
      "1563/1563 [==============================] - 24s 15ms/step - loss: 0.9086 - accuracy: 0.6827 - val_loss: 1.0487 - val_accuracy: 0.6326\n",
      "Epoch 31/300\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 0.8987 - accuracy: 0.6880 - val_loss: 1.0258 - val_accuracy: 0.6397\n",
      "Epoch 32/300\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 0.8877 - accuracy: 0.6910 - val_loss: 1.0704 - val_accuracy: 0.6287\n",
      "Epoch 33/300\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 0.8772 - accuracy: 0.6956 - val_loss: 1.0435 - val_accuracy: 0.6398\n",
      "Epoch 34/300\n",
      "1563/1563 [==============================] - 24s 15ms/step - loss: 0.8670 - accuracy: 0.6973 - val_loss: 1.0207 - val_accuracy: 0.6481\n",
      "Epoch 35/300\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 0.8577 - accuracy: 0.7013 - val_loss: 1.0266 - val_accuracy: 0.6439\n",
      "Epoch 36/300\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 0.8487 - accuracy: 0.7056 - val_loss: 1.0220 - val_accuracy: 0.6500\n",
      "Epoch 37/300\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 0.8383 - accuracy: 0.7106 - val_loss: 1.0188 - val_accuracy: 0.6473\n",
      "Epoch 38/300\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 0.8304 - accuracy: 0.7113 - val_loss: 1.0171 - val_accuracy: 0.6527\n",
      "Epoch 39/300\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 0.8206 - accuracy: 0.7149 - val_loss: 1.0312 - val_accuracy: 0.6483\n",
      "Epoch 40/300\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 0.8127 - accuracy: 0.7195 - val_loss: 1.0457 - val_accuracy: 0.6462\n",
      "Epoch 41/300\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 0.8031 - accuracy: 0.7202 - val_loss: 1.0119 - val_accuracy: 0.6537\n",
      "Epoch 42/300\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 0.7942 - accuracy: 0.7245 - val_loss: 1.0529 - val_accuracy: 0.6389\n",
      "Epoch 43/300\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 0.7866 - accuracy: 0.7268 - val_loss: 1.0020 - val_accuracy: 0.6536\n",
      "Epoch 44/300\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 0.7771 - accuracy: 0.7304 - val_loss: 1.0096 - val_accuracy: 0.6570\n",
      "Epoch 45/300\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 0.7697 - accuracy: 0.7337 - val_loss: 0.9997 - val_accuracy: 0.6598\n",
      "Epoch 46/300\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 0.7623 - accuracy: 0.7368 - val_loss: 1.0140 - val_accuracy: 0.6523\n",
      "Epoch 47/300\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 0.7536 - accuracy: 0.7404 - val_loss: 1.0121 - val_accuracy: 0.6553\n",
      "Epoch 48/300\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 0.7464 - accuracy: 0.7421 - val_loss: 1.0318 - val_accuracy: 0.6504\n",
      "Epoch 49/300\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 0.7393 - accuracy: 0.7450 - val_loss: 0.9888 - val_accuracy: 0.6666\n",
      "Epoch 50/300\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 0.7292 - accuracy: 0.7484 - val_loss: 1.0076 - val_accuracy: 0.6543\n",
      "Epoch 51/300\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 0.7248 - accuracy: 0.7505 - val_loss: 0.9873 - val_accuracy: 0.6659\n",
      "Epoch 52/300\n",
      "1563/1563 [==============================] - 24s 15ms/step - loss: 0.7162 - accuracy: 0.7518 - val_loss: 1.0133 - val_accuracy: 0.6633\n",
      "Epoch 53/300\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 0.7087 - accuracy: 0.7545 - val_loss: 1.0227 - val_accuracy: 0.6571\n",
      "Epoch 54/300\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 0.7018 - accuracy: 0.7582 - val_loss: 1.0025 - val_accuracy: 0.6581\n",
      "Epoch 55/300\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 0.6954 - accuracy: 0.7599 - val_loss: 1.0068 - val_accuracy: 0.6652\n",
      "Epoch 56/300\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 0.6883 - accuracy: 0.7619 - val_loss: 1.0007 - val_accuracy: 0.6641\n",
      "Epoch 57/300\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 0.6829 - accuracy: 0.7663 - val_loss: 1.0600 - val_accuracy: 0.6522\n",
      "Epoch 58/300\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 0.6756 - accuracy: 0.7662 - val_loss: 1.0408 - val_accuracy: 0.6537\n",
      "Epoch 59/300\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 0.6702 - accuracy: 0.7683 - val_loss: 1.0316 - val_accuracy: 0.6596\n",
      "Epoch 60/300\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 0.6622 - accuracy: 0.7718 - val_loss: 1.0128 - val_accuracy: 0.6636\n",
      "Epoch 61/300\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 0.6556 - accuracy: 0.7727 - val_loss: 1.0489 - val_accuracy: 0.6563\n",
      "Epoch 62/300\n",
      "1563/1563 [==============================] - 26s 17ms/step - loss: 0.6508 - accuracy: 0.7757 - val_loss: 1.0315 - val_accuracy: 0.6636\n",
      "Epoch 63/300\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 0.6427 - accuracy: 0.7791 - val_loss: 1.0247 - val_accuracy: 0.6651\n",
      "Epoch 64/300\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 0.6360 - accuracy: 0.7808 - val_loss: 1.0256 - val_accuracy: 0.6643\n",
      "Epoch 65/300\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 0.6306 - accuracy: 0.7838 - val_loss: 1.0601 - val_accuracy: 0.6543\n",
      "Epoch 66/300\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 0.6253 - accuracy: 0.7840 - val_loss: 1.0160 - val_accuracy: 0.6692\n",
      "Epoch 67/300\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 0.6199 - accuracy: 0.7861 - val_loss: 1.0604 - val_accuracy: 0.6584\n",
      "Epoch 68/300\n",
      "1563/1563 [==============================] - 23s 14ms/step - loss: 0.6132 - accuracy: 0.7875 - val_loss: 1.0300 - val_accuracy: 0.6662\n",
      "Epoch 69/300\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 0.6071 - accuracy: 0.7909 - val_loss: 1.0588 - val_accuracy: 0.6571\n",
      "Epoch 70/300\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 0.6005 - accuracy: 0.7948 - val_loss: 1.0293 - val_accuracy: 0.6708\n",
      "Epoch 71/300\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 0.5951 - accuracy: 0.7957 - val_loss: 1.0445 - val_accuracy: 0.6677\n",
      "Epoch 72/300\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 0.5892 - accuracy: 0.7971 - val_loss: 1.0751 - val_accuracy: 0.6603\n",
      "Epoch 73/300\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 0.5837 - accuracy: 0.7996 - val_loss: 1.0623 - val_accuracy: 0.6617\n",
      "Epoch 74/300\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 0.5778 - accuracy: 0.8039 - val_loss: 1.0632 - val_accuracy: 0.6654\n",
      "Epoch 75/300\n",
      "1563/1563 [==============================] - 23s 14ms/step - loss: 0.5724 - accuracy: 0.8030 - val_loss: 1.0810 - val_accuracy: 0.6622\n",
      "Epoch 76/300\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 0.5663 - accuracy: 0.8058 - val_loss: 1.0619 - val_accuracy: 0.6638\n",
      "Epoch 77/300\n",
      "1563/1563 [==============================] - 23s 14ms/step - loss: 0.5605 - accuracy: 0.8081 - val_loss: 1.0927 - val_accuracy: 0.6590\n",
      "Epoch 78/300\n",
      "1563/1563 [==============================] - 24s 15ms/step - loss: 0.5557 - accuracy: 0.8086 - val_loss: 1.0693 - val_accuracy: 0.6606\n",
      "Epoch 79/300\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 0.5496 - accuracy: 0.8117 - val_loss: 1.0849 - val_accuracy: 0.6635\n",
      "Epoch 80/300\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 0.5453 - accuracy: 0.8127 - val_loss: 1.0776 - val_accuracy: 0.6625\n",
      "Epoch 81/300\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 0.5388 - accuracy: 0.8147 - val_loss: 1.1203 - val_accuracy: 0.6614\n",
      "Epoch 82/300\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 0.5341 - accuracy: 0.8168 - val_loss: 1.1190 - val_accuracy: 0.6585\n",
      "Epoch 83/300\n",
      "1563/1563 [==============================] - 24s 16ms/step - loss: 0.5286 - accuracy: 0.8182 - val_loss: 1.1015 - val_accuracy: 0.6650\n",
      "Epoch 84/300\n",
      "1563/1563 [==============================] - 23s 14ms/step - loss: 0.5231 - accuracy: 0.8208 - val_loss: 1.1374 - val_accuracy: 0.6616\n",
      "Epoch 85/300\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 0.5187 - accuracy: 0.8228 - val_loss: 1.0985 - val_accuracy: 0.6672\n",
      "Epoch 86/300\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 0.5126 - accuracy: 0.8241 - val_loss: 1.1440 - val_accuracy: 0.6614\n",
      "Epoch 87/300\n",
      "1563/1563 [==============================] - 23s 14ms/step - loss: 0.5086 - accuracy: 0.8239 - val_loss: 1.1082 - val_accuracy: 0.6651\n",
      "Epoch 88/300\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 0.5017 - accuracy: 0.8275 - val_loss: 1.1746 - val_accuracy: 0.6556\n",
      "Epoch 89/300\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 0.4971 - accuracy: 0.8299 - val_loss: 1.1378 - val_accuracy: 0.6623\n",
      "Epoch 90/300\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 0.4924 - accuracy: 0.8314 - val_loss: 1.1394 - val_accuracy: 0.6646\n",
      "Epoch 91/300\n",
      "1563/1563 [==============================] - 24s 15ms/step - loss: 0.4884 - accuracy: 0.8334 - val_loss: 1.1542 - val_accuracy: 0.6576\n",
      "Epoch 92/300\n",
      "1563/1563 [==============================] - 24s 15ms/step - loss: 0.4842 - accuracy: 0.8344 - val_loss: 1.1506 - val_accuracy: 0.6631\n",
      "Epoch 93/300\n",
      "1563/1563 [==============================] - 24s 15ms/step - loss: 0.4782 - accuracy: 0.8359 - val_loss: 1.1776 - val_accuracy: 0.6550\n",
      "Epoch 94/300\n",
      "1563/1563 [==============================] - 24s 15ms/step - loss: 0.4714 - accuracy: 0.8385 - val_loss: 1.1716 - val_accuracy: 0.6609\n",
      "Epoch 95/300\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 0.4697 - accuracy: 0.8392 - val_loss: 1.1723 - val_accuracy: 0.6626\n",
      "Epoch 96/300\n",
      "1563/1563 [==============================] - 24s 15ms/step - loss: 0.4637 - accuracy: 0.8421 - val_loss: 1.1791 - val_accuracy: 0.6601\n",
      "Epoch 97/300\n",
      "1563/1563 [==============================] - 24s 15ms/step - loss: 0.4588 - accuracy: 0.8439 - val_loss: 1.1708 - val_accuracy: 0.6630\n",
      "Epoch 98/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1563/1563 [==============================] - 24s 15ms/step - loss: 0.4541 - accuracy: 0.8447 - val_loss: 1.1980 - val_accuracy: 0.6577\n",
      "Epoch 99/300\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 0.4502 - accuracy: 0.8452 - val_loss: 1.2082 - val_accuracy: 0.6558\n",
      "Epoch 100/300\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 0.4447 - accuracy: 0.8490 - val_loss: 1.2008 - val_accuracy: 0.6600\n",
      "Epoch 101/300\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 0.4391 - accuracy: 0.8497 - val_loss: 1.2353 - val_accuracy: 0.6555\n",
      "Epoch 102/300\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 0.4357 - accuracy: 0.8519 - val_loss: 1.2426 - val_accuracy: 0.6532\n",
      "Epoch 103/300\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 0.4314 - accuracy: 0.8526 - val_loss: 1.2080 - val_accuracy: 0.6636\n",
      "Epoch 104/300\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 0.4268 - accuracy: 0.8537 - val_loss: 1.2408 - val_accuracy: 0.6612\n",
      "Epoch 105/300\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 0.4217 - accuracy: 0.8553 - val_loss: 1.2402 - val_accuracy: 0.6590\n",
      "Epoch 106/300\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 0.4170 - accuracy: 0.8575 - val_loss: 1.2846 - val_accuracy: 0.6595\n",
      "Epoch 107/300\n",
      "1563/1563 [==============================] - 24s 16ms/step - loss: 0.4123 - accuracy: 0.8603 - val_loss: 1.2764 - val_accuracy: 0.6614\n",
      "Epoch 108/300\n",
      "1563/1563 [==============================] - 24s 16ms/step - loss: 0.4086 - accuracy: 0.8612 - val_loss: 1.2761 - val_accuracy: 0.6590\n",
      "Epoch 109/300\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 0.4041 - accuracy: 0.8622 - val_loss: 1.2796 - val_accuracy: 0.6581\n",
      "Epoch 110/300\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 0.4002 - accuracy: 0.8657 - val_loss: 1.2978 - val_accuracy: 0.6637\n",
      "Epoch 111/300\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 0.3952 - accuracy: 0.8652 - val_loss: 1.3093 - val_accuracy: 0.6555\n",
      "Epoch 112/300\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 0.3918 - accuracy: 0.8660 - val_loss: 1.3021 - val_accuracy: 0.6562\n",
      "Epoch 113/300\n",
      "1563/1563 [==============================] - 24s 16ms/step - loss: 0.3874 - accuracy: 0.8685 - val_loss: 1.3178 - val_accuracy: 0.6598\n",
      "Epoch 114/300\n",
      "1563/1563 [==============================] - 24s 15ms/step - loss: 0.3824 - accuracy: 0.8708 - val_loss: 1.3285 - val_accuracy: 0.6610\n",
      "Epoch 115/300\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 0.3795 - accuracy: 0.8701 - val_loss: 1.3518 - val_accuracy: 0.6571\n",
      "Epoch 116/300\n",
      "1563/1563 [==============================] - 24s 15ms/step - loss: 0.3744 - accuracy: 0.8725 - val_loss: 1.3332 - val_accuracy: 0.6561\n",
      "Epoch 117/300\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 0.3707 - accuracy: 0.8744 - val_loss: 1.4048 - val_accuracy: 0.6497\n",
      "Epoch 118/300\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 0.3665 - accuracy: 0.8756 - val_loss: 1.3403 - val_accuracy: 0.6601\n",
      "Epoch 119/300\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 0.3618 - accuracy: 0.8784 - val_loss: 1.3952 - val_accuracy: 0.6560\n",
      "Epoch 120/300\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 0.3572 - accuracy: 0.8775 - val_loss: 1.3977 - val_accuracy: 0.6540\n",
      "Epoch 121/300\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 0.3542 - accuracy: 0.8795 - val_loss: 1.3784 - val_accuracy: 0.6584\n",
      "Epoch 122/300\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 0.3495 - accuracy: 0.8819 - val_loss: 1.3894 - val_accuracy: 0.6619\n",
      "Epoch 123/300\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 0.3461 - accuracy: 0.8832 - val_loss: 1.4260 - val_accuracy: 0.6471\n",
      "Epoch 124/300\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 0.3416 - accuracy: 0.8845 - val_loss: 1.4203 - val_accuracy: 0.6565\n",
      "Epoch 125/300\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 0.3372 - accuracy: 0.8862 - val_loss: 1.4289 - val_accuracy: 0.6532\n",
      "Epoch 126/300\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 0.3337 - accuracy: 0.8882 - val_loss: 1.4413 - val_accuracy: 0.6551\n",
      "Epoch 127/300\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 0.3296 - accuracy: 0.8888 - val_loss: 1.4481 - val_accuracy: 0.6529\n",
      "Epoch 128/300\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 0.3258 - accuracy: 0.8893 - val_loss: 1.4499 - val_accuracy: 0.6538\n",
      "Epoch 129/300\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 0.3223 - accuracy: 0.8905 - val_loss: 1.4495 - val_accuracy: 0.6553\n",
      "Epoch 130/300\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 0.3182 - accuracy: 0.8931 - val_loss: 1.4641 - val_accuracy: 0.6589\n",
      "Epoch 131/300\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 0.3144 - accuracy: 0.8941 - val_loss: 1.5279 - val_accuracy: 0.6526\n",
      "Epoch 132/300\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 0.3117 - accuracy: 0.8949 - val_loss: 1.5139 - val_accuracy: 0.6549\n",
      "Epoch 133/300\n",
      "1563/1563 [==============================] - 23s 14ms/step - loss: 0.3075 - accuracy: 0.8970 - val_loss: 1.5130 - val_accuracy: 0.6555\n",
      "Epoch 134/300\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 0.3036 - accuracy: 0.8984 - val_loss: 1.5414 - val_accuracy: 0.6531\n",
      "Epoch 135/300\n",
      "1563/1563 [==============================] - 24s 15ms/step - loss: 0.2987 - accuracy: 0.9001 - val_loss: 1.5477 - val_accuracy: 0.6533\n",
      "Epoch 136/300\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 0.2952 - accuracy: 0.9016 - val_loss: 1.5533 - val_accuracy: 0.6498\n",
      "Epoch 137/300\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 0.2929 - accuracy: 0.9021 - val_loss: 1.5583 - val_accuracy: 0.6510\n",
      "Epoch 138/300\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 0.2887 - accuracy: 0.9037 - val_loss: 1.5729 - val_accuracy: 0.6513\n",
      "Epoch 139/300\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 0.2848 - accuracy: 0.9053 - val_loss: 1.5826 - val_accuracy: 0.6514\n",
      "Epoch 140/300\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 0.2826 - accuracy: 0.9057 - val_loss: 1.5962 - val_accuracy: 0.6537\n",
      "Epoch 141/300\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 0.2785 - accuracy: 0.9073 - val_loss: 1.5814 - val_accuracy: 0.6478\n",
      "Epoch 142/300\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 0.2744 - accuracy: 0.9080 - val_loss: 1.6421 - val_accuracy: 0.6535\n",
      "Epoch 143/300\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 0.2697 - accuracy: 0.9108 - val_loss: 1.6444 - val_accuracy: 0.6516\n",
      "Epoch 144/300\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 0.2675 - accuracy: 0.9114 - val_loss: 1.6337 - val_accuracy: 0.6483\n",
      "Epoch 145/300\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 0.2650 - accuracy: 0.9115 - val_loss: 1.6804 - val_accuracy: 0.6472\n",
      "Epoch 146/300\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 0.2594 - accuracy: 0.9141 - val_loss: 1.7163 - val_accuracy: 0.6484\n",
      "Epoch 147/300\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 0.2571 - accuracy: 0.9152 - val_loss: 1.6873 - val_accuracy: 0.6539\n",
      "Epoch 148/300\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 0.2527 - accuracy: 0.9170 - val_loss: 1.6913 - val_accuracy: 0.6542\n",
      "Epoch 149/300\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 0.2501 - accuracy: 0.9189 - val_loss: 1.7432 - val_accuracy: 0.6510\n",
      "Epoch 150/300\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 0.2472 - accuracy: 0.9189 - val_loss: 1.7467 - val_accuracy: 0.6545\n",
      "Epoch 151/300\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 0.2427 - accuracy: 0.9189 - val_loss: 1.7309 - val_accuracy: 0.6525\n",
      "Epoch 152/300\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 0.2407 - accuracy: 0.9211 - val_loss: 1.7472 - val_accuracy: 0.6480\n",
      "Epoch 153/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1563/1563 [==============================] - 23s 15ms/step - loss: 0.2384 - accuracy: 0.9218 - val_loss: 1.7787 - val_accuracy: 0.6492\n",
      "Epoch 154/300\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 0.2340 - accuracy: 0.9233 - val_loss: 1.7988 - val_accuracy: 0.6419\n",
      "Epoch 155/300\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 0.2303 - accuracy: 0.9249 - val_loss: 1.8002 - val_accuracy: 0.6508\n",
      "Epoch 156/300\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 0.2274 - accuracy: 0.9257 - val_loss: 1.8439 - val_accuracy: 0.6440\n",
      "Epoch 157/300\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 0.2249 - accuracy: 0.9262 - val_loss: 1.8089 - val_accuracy: 0.6454\n",
      "Epoch 158/300\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 0.2208 - accuracy: 0.9282 - val_loss: 1.8812 - val_accuracy: 0.6473\n",
      "Epoch 159/300\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 0.2181 - accuracy: 0.9292 - val_loss: 1.8710 - val_accuracy: 0.6465\n",
      "Epoch 160/300\n",
      "1563/1563 [==============================] - 23s 14ms/step - loss: 0.2152 - accuracy: 0.9297 - val_loss: 1.9285 - val_accuracy: 0.6469\n",
      "Epoch 161/300\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 0.2123 - accuracy: 0.9322 - val_loss: 1.8848 - val_accuracy: 0.6484\n",
      "Epoch 162/300\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 0.2098 - accuracy: 0.9320 - val_loss: 1.9258 - val_accuracy: 0.6462\n",
      "Epoch 163/300\n",
      "1563/1563 [==============================] - 26s 16ms/step - loss: 0.2055 - accuracy: 0.9339 - val_loss: 1.9162 - val_accuracy: 0.6439\n",
      "Epoch 164/300\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 0.2029 - accuracy: 0.9347 - val_loss: 1.9375 - val_accuracy: 0.6469\n",
      "Epoch 165/300\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 0.1986 - accuracy: 0.9358 - val_loss: 1.9494 - val_accuracy: 0.6428\n",
      "Epoch 166/300\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 0.1967 - accuracy: 0.9368 - val_loss: 2.0151 - val_accuracy: 0.6399\n",
      "Epoch 167/300\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 0.1943 - accuracy: 0.9384 - val_loss: 1.9876 - val_accuracy: 0.6407\n",
      "Epoch 168/300\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 0.1914 - accuracy: 0.9388 - val_loss: 1.9847 - val_accuracy: 0.6418\n",
      "Epoch 169/300\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 0.1890 - accuracy: 0.9396 - val_loss: 1.9868 - val_accuracy: 0.6480\n",
      "Epoch 170/300\n",
      "1563/1563 [==============================] - 24s 15ms/step - loss: 0.1853 - accuracy: 0.9397 - val_loss: 2.0350 - val_accuracy: 0.6375\n",
      "Epoch 171/300\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 0.1835 - accuracy: 0.9414 - val_loss: 2.0710 - val_accuracy: 0.6474\n",
      "Epoch 172/300\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 0.1795 - accuracy: 0.9437 - val_loss: 2.0973 - val_accuracy: 0.6396\n",
      "Epoch 173/300\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 0.1758 - accuracy: 0.9443 - val_loss: 2.0984 - val_accuracy: 0.6476\n",
      "Epoch 174/300\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 0.1752 - accuracy: 0.9447 - val_loss: 2.1068 - val_accuracy: 0.6475\n",
      "Epoch 175/300\n",
      "1563/1563 [==============================] - 24s 15ms/step - loss: 0.1714 - accuracy: 0.9447 - val_loss: 2.1400 - val_accuracy: 0.6386\n",
      "Epoch 176/300\n",
      "1563/1563 [==============================] - 24s 15ms/step - loss: 0.1681 - accuracy: 0.9460 - val_loss: 2.1226 - val_accuracy: 0.6433\n",
      "Epoch 177/300\n",
      "1563/1563 [==============================] - 24s 15ms/step - loss: 0.1673 - accuracy: 0.9462 - val_loss: 2.1395 - val_accuracy: 0.6446\n",
      "Epoch 178/300\n",
      "1563/1563 [==============================] - 24s 15ms/step - loss: 0.1637 - accuracy: 0.9480 - val_loss: 2.1929 - val_accuracy: 0.6437\n",
      "Epoch 179/300\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 0.1613 - accuracy: 0.9492 - val_loss: 2.1875 - val_accuracy: 0.6447\n",
      "Epoch 180/300\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 0.1587 - accuracy: 0.9490 - val_loss: 2.1938 - val_accuracy: 0.6426\n",
      "Epoch 181/300\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 0.1557 - accuracy: 0.9521 - val_loss: 2.2320 - val_accuracy: 0.6446\n",
      "Epoch 182/300\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 0.1535 - accuracy: 0.9520 - val_loss: 2.2177 - val_accuracy: 0.6443\n",
      "Epoch 183/300\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 0.1507 - accuracy: 0.9536 - val_loss: 2.2344 - val_accuracy: 0.6416\n",
      "Epoch 184/300\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 0.1489 - accuracy: 0.9536 - val_loss: 2.2831 - val_accuracy: 0.6423\n",
      "Epoch 185/300\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 0.1463 - accuracy: 0.9547 - val_loss: 2.3190 - val_accuracy: 0.6411\n",
      "Epoch 186/300\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 0.1443 - accuracy: 0.9550 - val_loss: 2.3211 - val_accuracy: 0.6367\n",
      "Epoch 187/300\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 0.1402 - accuracy: 0.9560 - val_loss: 2.3287 - val_accuracy: 0.6400\n",
      "Epoch 188/300\n",
      "1563/1563 [==============================] - 24s 15ms/step - loss: 0.1386 - accuracy: 0.9571 - val_loss: 2.3587 - val_accuracy: 0.6415\n",
      "Epoch 189/300\n",
      "1563/1563 [==============================] - 24s 15ms/step - loss: 0.1362 - accuracy: 0.9584 - val_loss: 2.3767 - val_accuracy: 0.6421\n",
      "Epoch 190/300\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 0.1344 - accuracy: 0.9585 - val_loss: 2.4619 - val_accuracy: 0.6345\n",
      "Epoch 191/300\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 0.1328 - accuracy: 0.9590 - val_loss: 2.4509 - val_accuracy: 0.6445\n",
      "Epoch 192/300\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 0.1295 - accuracy: 0.9602 - val_loss: 2.4452 - val_accuracy: 0.6448\n",
      "Epoch 193/300\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 0.1279 - accuracy: 0.9606 - val_loss: 2.4553 - val_accuracy: 0.6360\n",
      "Epoch 194/300\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 0.1241 - accuracy: 0.9625 - val_loss: 2.4446 - val_accuracy: 0.6411\n",
      "Epoch 195/300\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 0.1241 - accuracy: 0.9626 - val_loss: 2.4895 - val_accuracy: 0.6397\n",
      "Epoch 196/300\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 0.1215 - accuracy: 0.9626 - val_loss: 2.5045 - val_accuracy: 0.6441\n",
      "Epoch 197/300\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 0.1195 - accuracy: 0.9640 - val_loss: 2.5455 - val_accuracy: 0.6413\n",
      "Epoch 198/300\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 0.1166 - accuracy: 0.9641 - val_loss: 2.5697 - val_accuracy: 0.6342\n",
      "Epoch 199/300\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 0.1153 - accuracy: 0.9650 - val_loss: 2.5860 - val_accuracy: 0.6358\n",
      "Epoch 200/300\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 0.1124 - accuracy: 0.9660 - val_loss: 2.6311 - val_accuracy: 0.6370\n",
      "Epoch 201/300\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 0.1119 - accuracy: 0.9656 - val_loss: 2.6186 - val_accuracy: 0.6376\n",
      "Epoch 202/300\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 0.1088 - accuracy: 0.9678 - val_loss: 2.6501 - val_accuracy: 0.6358\n",
      "Epoch 203/300\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 0.1071 - accuracy: 0.9677 - val_loss: 2.6409 - val_accuracy: 0.6401\n",
      "Epoch 204/300\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 0.1066 - accuracy: 0.9671 - val_loss: 2.7126 - val_accuracy: 0.6352\n",
      "Epoch 205/300\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 0.1029 - accuracy: 0.9699 - val_loss: 2.7360 - val_accuracy: 0.6385\n",
      "Epoch 206/300\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 0.1014 - accuracy: 0.9700 - val_loss: 2.7336 - val_accuracy: 0.6382\n",
      "Epoch 207/300\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 0.0990 - accuracy: 0.9710 - val_loss: 2.7596 - val_accuracy: 0.6387\n",
      "Epoch 208/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1563/1563 [==============================] - 24s 15ms/step - loss: 0.0982 - accuracy: 0.9711 - val_loss: 2.7638 - val_accuracy: 0.6418\n",
      "Epoch 209/300\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 0.0964 - accuracy: 0.9717 - val_loss: 2.8086 - val_accuracy: 0.6375\n",
      "Epoch 210/300\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 0.0933 - accuracy: 0.9729 - val_loss: 2.7565 - val_accuracy: 0.6376\n",
      "Epoch 211/300\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 0.0935 - accuracy: 0.9721 - val_loss: 2.8047 - val_accuracy: 0.6357\n",
      "Epoch 212/300\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 0.0903 - accuracy: 0.9735 - val_loss: 2.8324 - val_accuracy: 0.6415\n",
      "Epoch 213/300\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 0.0900 - accuracy: 0.9735 - val_loss: 2.8617 - val_accuracy: 0.6382\n",
      "Epoch 214/300\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 0.0870 - accuracy: 0.9751 - val_loss: 2.9084 - val_accuracy: 0.6397\n",
      "Epoch 215/300\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 0.0856 - accuracy: 0.9753 - val_loss: 2.8765 - val_accuracy: 0.6382\n",
      "Epoch 216/300\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 0.0839 - accuracy: 0.9757 - val_loss: 2.9351 - val_accuracy: 0.6373\n",
      "Epoch 217/300\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 0.0822 - accuracy: 0.9769 - val_loss: 2.9930 - val_accuracy: 0.6370\n",
      "Epoch 218/300\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 0.0811 - accuracy: 0.9760 - val_loss: 2.9980 - val_accuracy: 0.6386\n",
      "Epoch 219/300\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 0.0791 - accuracy: 0.9781 - val_loss: 2.9754 - val_accuracy: 0.6353\n",
      "Epoch 220/300\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 0.0781 - accuracy: 0.9781 - val_loss: 3.0932 - val_accuracy: 0.6351\n",
      "Epoch 221/300\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 0.0772 - accuracy: 0.9780 - val_loss: 3.1024 - val_accuracy: 0.6365\n",
      "Epoch 222/300\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 0.0758 - accuracy: 0.9790 - val_loss: 3.0666 - val_accuracy: 0.6365\n",
      "Epoch 223/300\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 0.0738 - accuracy: 0.9788 - val_loss: 3.1707 - val_accuracy: 0.6336\n",
      "Epoch 224/300\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 0.0726 - accuracy: 0.9790 - val_loss: 3.1349 - val_accuracy: 0.6390\n",
      "Epoch 225/300\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 0.0708 - accuracy: 0.9806 - val_loss: 3.1799 - val_accuracy: 0.6387\n",
      "Epoch 226/300\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 0.0695 - accuracy: 0.9804 - val_loss: 3.2205 - val_accuracy: 0.6367\n",
      "Epoch 227/300\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 0.0677 - accuracy: 0.9807 - val_loss: 3.1853 - val_accuracy: 0.6385\n",
      "Epoch 228/300\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 0.0678 - accuracy: 0.9806 - val_loss: 3.2078 - val_accuracy: 0.6357\n",
      "Epoch 229/300\n",
      "1563/1563 [==============================] - 24s 15ms/step - loss: 0.0653 - accuracy: 0.9824 - val_loss: 3.2891 - val_accuracy: 0.6332\n",
      "Epoch 230/300\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 0.0645 - accuracy: 0.9820 - val_loss: 3.3159 - val_accuracy: 0.6359\n",
      "Epoch 231/300\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 0.0632 - accuracy: 0.9822 - val_loss: 3.2750 - val_accuracy: 0.6375\n",
      "Epoch 232/300\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 0.0619 - accuracy: 0.9831 - val_loss: 3.3474 - val_accuracy: 0.6383\n",
      "Epoch 233/300\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 0.0599 - accuracy: 0.9841 - val_loss: 3.3806 - val_accuracy: 0.6304\n",
      "Epoch 234/300\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 0.0587 - accuracy: 0.9850 - val_loss: 3.4191 - val_accuracy: 0.6327\n",
      "Epoch 235/300\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 0.0583 - accuracy: 0.9842 - val_loss: 3.3618 - val_accuracy: 0.6389\n",
      "Epoch 236/300\n",
      "1563/1563 [==============================] - 22s 14ms/step - loss: 0.0574 - accuracy: 0.9841 - val_loss: 3.4177 - val_accuracy: 0.6349\n",
      "Epoch 237/300\n",
      "1563/1563 [==============================] - 21s 14ms/step - loss: 0.0565 - accuracy: 0.9846 - val_loss: 3.4215 - val_accuracy: 0.6374\n",
      "Epoch 238/300\n",
      "1563/1563 [==============================] - 22s 14ms/step - loss: 0.0552 - accuracy: 0.9847 - val_loss: 3.5701 - val_accuracy: 0.6296\n",
      "Epoch 239/300\n",
      "1563/1563 [==============================] - 21s 14ms/step - loss: 0.0535 - accuracy: 0.9857 - val_loss: 3.4666 - val_accuracy: 0.6353\n",
      "Epoch 240/300\n",
      "1563/1563 [==============================] - 22s 14ms/step - loss: 0.0528 - accuracy: 0.9860 - val_loss: 3.4813 - val_accuracy: 0.6380\n",
      "Epoch 241/300\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 0.0530 - accuracy: 0.9854 - val_loss: 3.5906 - val_accuracy: 0.6336\n",
      "Epoch 242/300\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 0.0512 - accuracy: 0.9866 - val_loss: 3.5447 - val_accuracy: 0.6378\n",
      "Epoch 243/300\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 0.0501 - accuracy: 0.9871 - val_loss: 3.6367 - val_accuracy: 0.6359\n",
      "Epoch 244/300\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 0.0496 - accuracy: 0.9867 - val_loss: 3.5832 - val_accuracy: 0.6337\n",
      "Epoch 245/300\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 0.0484 - accuracy: 0.9872 - val_loss: 3.6263 - val_accuracy: 0.6373\n",
      "Epoch 246/300\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 0.0475 - accuracy: 0.9875 - val_loss: 3.6452 - val_accuracy: 0.6336\n",
      "Epoch 247/300\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 0.0466 - accuracy: 0.9880 - val_loss: 3.6539 - val_accuracy: 0.6379\n",
      "Epoch 248/300\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 0.0448 - accuracy: 0.9883 - val_loss: 3.6898 - val_accuracy: 0.6361\n",
      "Epoch 249/300\n",
      "1563/1563 [==============================] - 24s 15ms/step - loss: 0.0450 - accuracy: 0.9880 - val_loss: 3.7798 - val_accuracy: 0.6372\n",
      "Epoch 250/300\n",
      "1563/1563 [==============================] - 22s 14ms/step - loss: 0.0441 - accuracy: 0.9888 - val_loss: 3.7710 - val_accuracy: 0.6366\n",
      "Epoch 251/300\n",
      "1563/1563 [==============================] - 22s 14ms/step - loss: 0.0425 - accuracy: 0.9889 - val_loss: 3.8012 - val_accuracy: 0.6341\n",
      "Epoch 252/300\n",
      "1563/1563 [==============================] - 22s 14ms/step - loss: 0.0418 - accuracy: 0.9896 - val_loss: 3.7790 - val_accuracy: 0.6350\n",
      "Epoch 253/300\n",
      "1563/1563 [==============================] - 22s 14ms/step - loss: 0.0409 - accuracy: 0.9900 - val_loss: 3.8492 - val_accuracy: 0.6335\n",
      "Epoch 254/300\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 0.0407 - accuracy: 0.9897 - val_loss: 3.8273 - val_accuracy: 0.6329\n",
      "Epoch 255/300\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 0.0392 - accuracy: 0.9899 - val_loss: 3.9179 - val_accuracy: 0.6367\n",
      "Epoch 256/300\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 0.0397 - accuracy: 0.9899 - val_loss: 3.9137 - val_accuracy: 0.6308\n",
      "Epoch 257/300\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 0.0391 - accuracy: 0.9899 - val_loss: 3.9308 - val_accuracy: 0.6332\n",
      "Epoch 258/300\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 0.0381 - accuracy: 0.9904 - val_loss: 3.9297 - val_accuracy: 0.6314\n",
      "Epoch 259/300\n",
      "1563/1563 [==============================] - 22s 14ms/step - loss: 0.0376 - accuracy: 0.9907 - val_loss: 3.9760 - val_accuracy: 0.6324\n",
      "Epoch 260/300\n",
      "1563/1563 [==============================] - 22s 14ms/step - loss: 0.0367 - accuracy: 0.9908 - val_loss: 4.0049 - val_accuracy: 0.6382\n",
      "Epoch 261/300\n",
      "1563/1563 [==============================] - 22s 14ms/step - loss: 0.0362 - accuracy: 0.9907 - val_loss: 4.0503 - val_accuracy: 0.6337\n",
      "Epoch 262/300\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 0.0355 - accuracy: 0.9910 - val_loss: 4.0195 - val_accuracy: 0.6367\n",
      "Epoch 263/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1563/1563 [==============================] - 23s 15ms/step - loss: 0.0343 - accuracy: 0.9911 - val_loss: 4.0218 - val_accuracy: 0.6323\n",
      "Epoch 264/300\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 0.0332 - accuracy: 0.9914 - val_loss: 4.1144 - val_accuracy: 0.6343\n",
      "Epoch 265/300\n",
      "1563/1563 [==============================] - 22s 14ms/step - loss: 0.0335 - accuracy: 0.9918 - val_loss: 4.1111 - val_accuracy: 0.6356\n",
      "Epoch 266/300\n",
      "1563/1563 [==============================] - 23s 14ms/step - loss: 0.0323 - accuracy: 0.9916 - val_loss: 4.1137 - val_accuracy: 0.6351\n",
      "Epoch 267/300\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 0.0312 - accuracy: 0.9927 - val_loss: 4.1963 - val_accuracy: 0.6331\n",
      "Epoch 268/300\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 0.0319 - accuracy: 0.9921 - val_loss: 4.1499 - val_accuracy: 0.6378\n",
      "Epoch 269/300\n",
      "1563/1563 [==============================] - 23s 14ms/step - loss: 0.0309 - accuracy: 0.9922 - val_loss: 4.2152 - val_accuracy: 0.6361\n",
      "Epoch 270/300\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 0.0308 - accuracy: 0.9922 - val_loss: 4.2425 - val_accuracy: 0.6345\n",
      "Epoch 271/300\n",
      "1563/1563 [==============================] - 22s 14ms/step - loss: 0.0305 - accuracy: 0.9922 - val_loss: 4.2253 - val_accuracy: 0.6370\n",
      "Epoch 272/300\n",
      "1563/1563 [==============================] - 22s 14ms/step - loss: 0.0299 - accuracy: 0.9927 - val_loss: 4.2812 - val_accuracy: 0.6324\n",
      "Epoch 273/300\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 0.0291 - accuracy: 0.9926 - val_loss: 4.2860 - val_accuracy: 0.6328\n",
      "Epoch 274/300\n",
      "1563/1563 [==============================] - 23s 14ms/step - loss: 0.0291 - accuracy: 0.9929 - val_loss: 4.3080 - val_accuracy: 0.6375\n",
      "Epoch 275/300\n",
      "1563/1563 [==============================] - 22s 14ms/step - loss: 0.0290 - accuracy: 0.9927 - val_loss: 4.3490 - val_accuracy: 0.6349\n",
      "Epoch 276/300\n",
      "1563/1563 [==============================] - 23s 14ms/step - loss: 0.0281 - accuracy: 0.9932 - val_loss: 4.3994 - val_accuracy: 0.6374\n",
      "Epoch 277/300\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 0.0270 - accuracy: 0.9934 - val_loss: 4.3681 - val_accuracy: 0.6395\n",
      "Epoch 278/300\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 0.0263 - accuracy: 0.9937 - val_loss: 4.4206 - val_accuracy: 0.6386\n",
      "Epoch 279/300\n",
      "1563/1563 [==============================] - 22s 14ms/step - loss: 0.0255 - accuracy: 0.9937 - val_loss: 4.3697 - val_accuracy: 0.6360\n",
      "Epoch 280/300\n",
      "1563/1563 [==============================] - 22s 14ms/step - loss: 0.0256 - accuracy: 0.9934 - val_loss: 4.4281 - val_accuracy: 0.6359\n",
      "Epoch 281/300\n",
      "1563/1563 [==============================] - 23s 14ms/step - loss: 0.0255 - accuracy: 0.9934 - val_loss: 4.4371 - val_accuracy: 0.6381\n",
      "Epoch 282/300\n",
      "1563/1563 [==============================] - 22s 14ms/step - loss: 0.0255 - accuracy: 0.9935 - val_loss: 4.4739 - val_accuracy: 0.6371\n",
      "Epoch 283/300\n",
      "1563/1563 [==============================] - 23s 14ms/step - loss: 0.0253 - accuracy: 0.9939 - val_loss: 4.5879 - val_accuracy: 0.6331\n",
      "Epoch 284/300\n",
      "1563/1563 [==============================] - 22s 14ms/step - loss: 0.0242 - accuracy: 0.9942 - val_loss: 4.5022 - val_accuracy: 0.6381\n",
      "Epoch 285/300\n",
      "1563/1563 [==============================] - 22s 14ms/step - loss: 0.0244 - accuracy: 0.9939 - val_loss: 4.5310 - val_accuracy: 0.6374\n",
      "Epoch 286/300\n",
      "1563/1563 [==============================] - 22s 14ms/step - loss: 0.0250 - accuracy: 0.9933 - val_loss: 4.5915 - val_accuracy: 0.6373\n",
      "Epoch 287/300\n",
      "1563/1563 [==============================] - 22s 14ms/step - loss: 0.0223 - accuracy: 0.9944 - val_loss: 4.6027 - val_accuracy: 0.6404\n",
      "Epoch 288/300\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 0.0239 - accuracy: 0.9940 - val_loss: 4.5709 - val_accuracy: 0.6339\n",
      "Epoch 289/300\n",
      "1563/1563 [==============================] - 23s 14ms/step - loss: 0.0232 - accuracy: 0.9941 - val_loss: 4.6602 - val_accuracy: 0.6318\n",
      "Epoch 290/300\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 0.0224 - accuracy: 0.9948 - val_loss: 4.6773 - val_accuracy: 0.6337\n",
      "Epoch 291/300\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 0.0227 - accuracy: 0.9942 - val_loss: 4.6800 - val_accuracy: 0.6372\n",
      "Epoch 292/300\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 0.0215 - accuracy: 0.9946 - val_loss: 4.6678 - val_accuracy: 0.6370\n",
      "Epoch 293/300\n",
      "1563/1563 [==============================] - 22s 14ms/step - loss: 0.0217 - accuracy: 0.9946 - val_loss: 4.6903 - val_accuracy: 0.6370\n",
      "Epoch 294/300\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 0.0209 - accuracy: 0.9949 - val_loss: 4.7780 - val_accuracy: 0.6327\n",
      "Epoch 295/300\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 0.0216 - accuracy: 0.9943 - val_loss: 4.8255 - val_accuracy: 0.6338\n",
      "Epoch 296/300\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 0.0201 - accuracy: 0.9950 - val_loss: 4.8062 - val_accuracy: 0.6382\n",
      "Epoch 297/300\n",
      "1563/1563 [==============================] - 22s 14ms/step - loss: 0.0195 - accuracy: 0.9952 - val_loss: 4.8369 - val_accuracy: 0.6372\n",
      "Epoch 298/300\n",
      "1563/1563 [==============================] - 22s 14ms/step - loss: 0.0196 - accuracy: 0.9954 - val_loss: 4.8048 - val_accuracy: 0.6364\n",
      "Epoch 299/300\n",
      "1563/1563 [==============================] - 22s 14ms/step - loss: 0.0199 - accuracy: 0.9949 - val_loss: 4.8587 - val_accuracy: 0.6370\n",
      "Epoch 300/300\n",
      "1563/1563 [==============================] - 22s 14ms/step - loss: 0.0195 - accuracy: 0.9949 - val_loss: 5.0746 - val_accuracy: 0.6280\n",
      "WARNING:tensorflow:AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x00000222DFB0CF78> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Bad argument number for Name: 4, expecting 3\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x00000222DFB0CF78> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Bad argument number for Name: 4, expecting 3\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "INFO:tensorflow:Assets written to: models\\keras_cifar10_model_5_filtersize.h1\\assets\n",
      "CNN Model saved at models\\keras_cifar10_model_5_filtersize.h1 \n",
      "313/313 [==============================] - 1s 4ms/step - loss: 5.0746 - accuracy: 0.6280\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 0.0979 - accuracy: 0.9674\n",
      "Train loss:  0.097892627120018\n",
      "Trian metric:  0.9674199819564819\n",
      "Test loss:  5.074614524841309\n",
      "Test accuracy: 0.628000020980835\n",
      "{'loss': [1.9257813692092896, 1.6526703834533691, 1.5311790704727173, 1.454322099685669, 1.3981472253799438, 1.3496785163879395, 1.3086172342300415, 1.2721881866455078, 1.2404977083206177, 1.213058352470398, 1.189342737197876, 1.1657222509384155, 1.1447336673736572, 1.126596450805664, 1.1074718236923218, 1.0903161764144897, 1.0743155479431152, 1.0592159032821655, 1.04403555393219, 1.0298192501068115, 1.01617431640625, 1.0015655755996704, 0.9891075491905212, 0.9764683842658997, 0.9649314284324646, 0.952276349067688, 0.9410377144813538, 0.9298595786094666, 0.9189144372940063, 0.9085555076599121, 0.8986787796020508, 0.88772052526474, 0.8771719336509705, 0.8670294284820557, 0.8576554656028748, 0.8486900329589844, 0.8382811546325684, 0.8304455876350403, 0.8205657005310059, 0.8127118349075317, 0.8031044006347656, 0.7942168116569519, 0.7865604758262634, 0.7770860195159912, 0.7697474360466003, 0.7622936367988586, 0.7535976767539978, 0.7464136481285095, 0.739281415939331, 0.7292044162750244, 0.7248371839523315, 0.7161630392074585, 0.7087273597717285, 0.701809823513031, 0.695377767086029, 0.6883119940757751, 0.6828662753105164, 0.6756226420402527, 0.6702336668968201, 0.6622008085250854, 0.6556482315063477, 0.6507652997970581, 0.6426565051078796, 0.63601154088974, 0.6306002736091614, 0.6252887845039368, 0.6198761463165283, 0.6131929755210876, 0.607094943523407, 0.6005133986473083, 0.5951007008552551, 0.5892130136489868, 0.5837093591690063, 0.5778372287750244, 0.5723599791526794, 0.5662538409233093, 0.5605169534683228, 0.5557023882865906, 0.5496126413345337, 0.5453119277954102, 0.5387505888938904, 0.5341052412986755, 0.5285556316375732, 0.5231332182884216, 0.5187138319015503, 0.512633740901947, 0.508635938167572, 0.5016802549362183, 0.49714145064353943, 0.49235838651657104, 0.4883786141872406, 0.48416316509246826, 0.4782264530658722, 0.4713927209377289, 0.4696662425994873, 0.4637299180030823, 0.45875129103660583, 0.4540551006793976, 0.4501897394657135, 0.444694459438324, 0.43909117579460144, 0.43565985560417175, 0.4313521683216095, 0.42682385444641113, 0.42170387506484985, 0.41702815890312195, 0.4123110771179199, 0.40864384174346924, 0.40412142872810364, 0.40020400285720825, 0.3951950669288635, 0.39182791113853455, 0.3874478042125702, 0.3824371099472046, 0.379526287317276, 0.3743656277656555, 0.37069573998451233, 0.3665320575237274, 0.36178505420684814, 0.35721829533576965, 0.3541715741157532, 0.34947749972343445, 0.3460797965526581, 0.34161531925201416, 0.33721399307250977, 0.33367618918418884, 0.3296431601047516, 0.3258245885372162, 0.3223400413990021, 0.3181585967540741, 0.31440484523773193, 0.311691552400589, 0.3074868321418762, 0.3036419749259949, 0.2986592650413513, 0.29518234729766846, 0.29287612438201904, 0.28871938586235046, 0.28475555777549744, 0.2825700640678406, 0.2784830629825592, 0.2744072377681732, 0.2697121202945709, 0.2674899399280548, 0.2650325298309326, 0.2593839764595032, 0.25709137320518494, 0.252694696187973, 0.2501389682292938, 0.24723856151103973, 0.2427101880311966, 0.24073714017868042, 0.23842519521713257, 0.23403391242027283, 0.2303268015384674, 0.2274104654788971, 0.2249370664358139, 0.22083526849746704, 0.21812380850315094, 0.21516868472099304, 0.21226292848587036, 0.20982860028743744, 0.20550505816936493, 0.20287558436393738, 0.1986178457736969, 0.1966913938522339, 0.19428423047065735, 0.19140349328517914, 0.18900106847286224, 0.18529941141605377, 0.1834581047296524, 0.1795130968093872, 0.1758214682340622, 0.17516252398490906, 0.17140215635299683, 0.16811883449554443, 0.16729746758937836, 0.16368022561073303, 0.16131342947483063, 0.15874147415161133, 0.15570896863937378, 0.15351508557796478, 0.15067361295223236, 0.14894533157348633, 0.14628534018993378, 0.1443350464105606, 0.14020130038261414, 0.13858269155025482, 0.13620653748512268, 0.13435807824134827, 0.13282673060894012, 0.12952746450901031, 0.1278742402791977, 0.12412046641111374, 0.1240750253200531, 0.12152833491563797, 0.11948880553245544, 0.11662959307432175, 0.11529406905174255, 0.11241381615400314, 0.11185158044099808, 0.1088351309299469, 0.10713028162717819, 0.10664897412061691, 0.10288739204406738, 0.10143857449293137, 0.09896662086248398, 0.09818356484174728, 0.0963529646396637, 0.09330682456493378, 0.09354842454195023, 0.09031552076339722, 0.09003417938947678, 0.08699879050254822, 0.08556212484836578, 0.08393393456935883, 0.08221995085477829, 0.08107049763202667, 0.07910877466201782, 0.07813318073749542, 0.07715889066457748, 0.07576422393321991, 0.0738043487071991, 0.07257241010665894, 0.07076885551214218, 0.06949764490127563, 0.06766434758901596, 0.06783571094274521, 0.06528883427381516, 0.06451471894979477, 0.06320329755544662, 0.06189882010221481, 0.05988581106066704, 0.058734603226184845, 0.05826181173324585, 0.057429078966379166, 0.05651973560452461, 0.055229734629392624, 0.05349879339337349, 0.05279752239584923, 0.052958838641643524, 0.05116163194179535, 0.05013437196612358, 0.04957721754908562, 0.04842917248606682, 0.04745141789317131, 0.04664400592446327, 0.04477856308221817, 0.04499830678105354, 0.04411013796925545, 0.04248544201254845, 0.04178556054830551, 0.040868129581213, 0.04067510738968849, 0.03917087987065315, 0.03967301920056343, 0.03911370038986206, 0.03809322789311409, 0.037598855793476105, 0.03673240542411804, 0.03619519621133804, 0.035526275634765625, 0.03431861475110054, 0.03324834257364273, 0.033479735255241394, 0.032315194606781006, 0.031150583177804947, 0.03191211819648743, 0.03090803511440754, 0.03077060915529728, 0.030509887263178825, 0.029892345890402794, 0.029122568666934967, 0.029059182852506638, 0.029030002653598785, 0.02809881418943405, 0.026966920122504234, 0.02626417763531208, 0.02547852322459221, 0.02561100944876671, 0.02550463378429413, 0.025521906092762947, 0.02528771385550499, 0.02415868267416954, 0.024400627240538597, 0.024954035878181458, 0.022285474464297295, 0.02394101582467556, 0.023180965334177017, 0.022444071248173714, 0.02266799286007881, 0.021528730168938637, 0.021679751574993134, 0.020936181768774986, 0.021580511704087257, 0.020114921033382416, 0.01954675279557705, 0.019643332809209824, 0.019917042925953865, 0.019548233598470688], 'accuracy': [0.30831998586654663, 0.40738001465797424, 0.44944000244140625, 0.47637999057769775, 0.5007399916648865, 0.5192400217056274, 0.5359200239181519, 0.5487800240516663, 0.5621600151062012, 0.5708799958229065, 0.5802000164985657, 0.5882400274276733, 0.5943999886512756, 0.6043599843978882, 0.6109799742698669, 0.6158199906349182, 0.6210799813270569, 0.627560019493103, 0.632319986820221, 0.638480007648468, 0.6424999833106995, 0.6493800282478333, 0.6542199850082397, 0.6578800082206726, 0.6628599762916565, 0.667739987373352, 0.6710600256919861, 0.6730599999427795, 0.6798999905586243, 0.6827200055122375, 0.688040018081665, 0.6909599900245667, 0.6955999732017517, 0.6973000168800354, 0.7012799978256226, 0.7056000232696533, 0.7105799913406372, 0.7113199830055237, 0.7149400115013123, 0.7195000052452087, 0.7201799750328064, 0.7245399951934814, 0.7267799973487854, 0.7303799986839294, 0.7336599826812744, 0.736840009689331, 0.7404199838638306, 0.7420600056648254, 0.7450399994850159, 0.7483999729156494, 0.7505199909210205, 0.751800000667572, 0.7544800043106079, 0.7581999897956848, 0.7599200010299683, 0.7618600130081177, 0.7663000226020813, 0.7661600112915039, 0.7683200240135193, 0.7718200087547302, 0.7726600170135498, 0.7757400274276733, 0.7790600061416626, 0.7808200120925903, 0.7837799787521362, 0.7840200066566467, 0.7860999703407288, 0.7875199913978577, 0.7908999919891357, 0.7948200106620789, 0.7956799864768982, 0.7970600128173828, 0.7995799779891968, 0.8039399981498718, 0.8029999732971191, 0.8057799935340881, 0.8080800175666809, 0.8086199760437012, 0.811680018901825, 0.8126999735832214, 0.8147000074386597, 0.8168399930000305, 0.8181800246238708, 0.8208400011062622, 0.8228200078010559, 0.8241199851036072, 0.8238999843597412, 0.827459990978241, 0.8299199938774109, 0.8313800096511841, 0.8333799839019775, 0.8343999981880188, 0.835919976234436, 0.8385400176048279, 0.8391799926757812, 0.8420600295066833, 0.843940019607544, 0.8447200059890747, 0.8451799750328064, 0.8489599823951721, 0.8496800065040588, 0.851859986782074, 0.852620005607605, 0.8537399768829346, 0.8553199768066406, 0.857479989528656, 0.860260009765625, 0.8611800074577332, 0.8622000217437744, 0.8657000064849854, 0.8651800155639648, 0.8659999966621399, 0.8684800267219543, 0.8708199858665466, 0.870140016078949, 0.8724600076675415, 0.8744199872016907, 0.8755599856376648, 0.8783800005912781, 0.8775399923324585, 0.8795199990272522, 0.8818600177764893, 0.8832200169563293, 0.8845000267028809, 0.8861799836158752, 0.8881800174713135, 0.8887799978256226, 0.8892800211906433, 0.8905400037765503, 0.8931400179862976, 0.8940799832344055, 0.8949199914932251, 0.8969600200653076, 0.8983799815177917, 0.9001200199127197, 0.9015600085258484, 0.9021400213241577, 0.903659999370575, 0.9052600264549255, 0.9056800007820129, 0.9072800278663635, 0.9079999923706055, 0.9107599854469299, 0.9114400148391724, 0.9115399718284607, 0.914139986038208, 0.9151600003242493, 0.9169800281524658, 0.9188600182533264, 0.9189000129699707, 0.9189199805259705, 0.9210799932479858, 0.9218000173568726, 0.923259973526001, 0.9248600006103516, 0.9257400035858154, 0.9262400269508362, 0.9281799793243408, 0.9291599988937378, 0.9297199845314026, 0.9321600198745728, 0.9319999814033508, 0.9339200258255005, 0.934719979763031, 0.9358400106430054, 0.9368199706077576, 0.9383599758148193, 0.9387999773025513, 0.9396200180053711, 0.9396600127220154, 0.9413999915122986, 0.9436600208282471, 0.9442600011825562, 0.9447199702262878, 0.9446799755096436, 0.9459999799728394, 0.946179986000061, 0.9480400085449219, 0.9492200016975403, 0.9490200281143188, 0.9520599842071533, 0.9520000219345093, 0.9536200165748596, 0.9536200165748596, 0.9546800255775452, 0.9549599885940552, 0.9559999704360962, 0.957099974155426, 0.9584400057792664, 0.9584599733352661, 0.9590200185775757, 0.9602199792861938, 0.9606000185012817, 0.9625399708747864, 0.9625999927520752, 0.9626399874687195, 0.9639800190925598, 0.9640799760818481, 0.9649800062179565, 0.9660000205039978, 0.9655600190162659, 0.9677600264549255, 0.9676799774169922, 0.9671400189399719, 0.9698600172996521, 0.9699599742889404, 0.9710000157356262, 0.9710800051689148, 0.9717400074005127, 0.9728999733924866, 0.9721199870109558, 0.9735199809074402, 0.9734799861907959, 0.9751200079917908, 0.9753000140190125, 0.9756799936294556, 0.976859986782074, 0.9759600162506104, 0.9780600070953369, 0.9780799746513367, 0.9779999852180481, 0.9790400266647339, 0.9787999987602234, 0.9790400266647339, 0.9806399941444397, 0.9804199934005737, 0.9806600213050842, 0.9805999994277954, 0.982420027256012, 0.9820200204849243, 0.9822199940681458, 0.9830600023269653, 0.9840599894523621, 0.9849600195884705, 0.984220027923584, 0.9841200113296509, 0.9845799803733826, 0.9846799969673157, 0.9857199788093567, 0.9859600067138672, 0.9854000210762024, 0.9865800142288208, 0.9871000051498413, 0.9866999983787537, 0.9872400164604187, 0.9875199794769287, 0.9880200028419495, 0.9883000254631042, 0.9880399703979492, 0.9887999892234802, 0.9889199733734131, 0.9896000027656555, 0.9900199770927429, 0.9897400140762329, 0.9898999929428101, 0.9898599982261658, 0.9898599982261658, 0.9904199838638306, 0.9906600117683411, 0.9907600283622742, 0.9907000064849854, 0.9909600019454956, 0.9910799860954285, 0.9914399981498718, 0.9917799830436707, 0.9915800094604492, 0.9926599860191345, 0.9920600056648254, 0.9922400116920471, 0.9921600222587585, 0.9922000169754028, 0.9927200078964233, 0.9926199913024902, 0.9929400086402893, 0.9926999807357788, 0.9932399988174438, 0.993399977684021, 0.9937000274658203, 0.9937400221824646, 0.9934399724006653, 0.9934399724006653, 0.9935200214385986, 0.9939200282096863, 0.9942399859428406, 0.9939000010490417, 0.9933199882507324, 0.9944199919700623, 0.9940199851989746, 0.9940599799156189, 0.9947999715805054, 0.9941800236701965, 0.9946200251579285, 0.9946399927139282, 0.994920015335083, 0.9943199753761292, 0.9950000047683716, 0.9951599836349487, 0.9953799843788147, 0.994920015335083, 0.9948800206184387], 'val_loss': [1.7271485328674316, 1.6202350854873657, 1.4749765396118164, 1.4282859563827515, 1.37319815158844, 1.354915738105774, 1.3105272054672241, 1.2699662446975708, 1.2591938972473145, 1.2692255973815918, 1.2107247114181519, 1.2054096460342407, 1.1750487089157104, 1.2064563035964966, 1.2209287881851196, 1.152706265449524, 1.1751035451889038, 1.1295597553253174, 1.1208895444869995, 1.111868143081665, 1.1845872402191162, 1.101809024810791, 1.1369163990020752, 1.0667392015457153, 1.0633705854415894, 1.0742238759994507, 1.071770191192627, 1.076209545135498, 1.1002845764160156, 1.0487067699432373, 1.0258420705795288, 1.0704383850097656, 1.0434587001800537, 1.0207337141036987, 1.026625633239746, 1.0219533443450928, 1.0188387632369995, 1.0171256065368652, 1.0312259197235107, 1.0456870794296265, 1.0118799209594727, 1.0529433488845825, 1.002025842666626, 1.009641170501709, 0.999657392501831, 1.0140380859375, 1.0121122598648071, 1.031778335571289, 0.9887654185295105, 1.0076018571853638, 0.9873484373092651, 1.0133370161056519, 1.0227439403533936, 1.002536416053772, 1.0067633390426636, 1.0007476806640625, 1.059993028640747, 1.040779948234558, 1.0316213369369507, 1.012799620628357, 1.0489060878753662, 1.0314719676971436, 1.0247235298156738, 1.0255719423294067, 1.0601083040237427, 1.0159744024276733, 1.0604418516159058, 1.0300291776657104, 1.0587819814682007, 1.0293009281158447, 1.044508695602417, 1.0751123428344727, 1.062261939048767, 1.0632293224334717, 1.080979824066162, 1.061904788017273, 1.0926605463027954, 1.069254755973816, 1.0848984718322754, 1.0775541067123413, 1.1203408241271973, 1.1189838647842407, 1.1015498638153076, 1.1374300718307495, 1.0984927415847778, 1.1440335512161255, 1.1082451343536377, 1.1746422052383423, 1.1378066539764404, 1.1393711566925049, 1.1542195081710815, 1.1506212949752808, 1.1775715351104736, 1.1716316938400269, 1.1722513437271118, 1.1790518760681152, 1.1707621812820435, 1.1979551315307617, 1.2082058191299438, 1.2008355855941772, 1.2353074550628662, 1.2426331043243408, 1.2080440521240234, 1.240820288658142, 1.2401869297027588, 1.2845795154571533, 1.2764161825180054, 1.2760783433914185, 1.279568076133728, 1.2977685928344727, 1.309250831604004, 1.3020756244659424, 1.3178412914276123, 1.3285183906555176, 1.3518122434616089, 1.3331841230392456, 1.4048328399658203, 1.3403030633926392, 1.3952444791793823, 1.3976753950119019, 1.3784102201461792, 1.3893837928771973, 1.4260081052780151, 1.4202961921691895, 1.4289004802703857, 1.4413400888442993, 1.4481244087219238, 1.4498705863952637, 1.4495047330856323, 1.4640858173370361, 1.5279442071914673, 1.5139360427856445, 1.5129629373550415, 1.5414419174194336, 1.5476932525634766, 1.5533238649368286, 1.5583226680755615, 1.5728596448898315, 1.582645058631897, 1.5961836576461792, 1.5814491510391235, 1.6420536041259766, 1.6443723440170288, 1.6337060928344727, 1.6804368495941162, 1.7163138389587402, 1.687328815460205, 1.6912949085235596, 1.7432196140289307, 1.7467433214187622, 1.7309480905532837, 1.7471519708633423, 1.7786647081375122, 1.7988477945327759, 1.8002030849456787, 1.8438695669174194, 1.8088784217834473, 1.8812494277954102, 1.870991587638855, 1.9284628629684448, 1.8848260641098022, 1.925815224647522, 1.9162094593048096, 1.9375238418579102, 1.9494088888168335, 2.0150787830352783, 1.9875948429107666, 1.9847365617752075, 1.9867851734161377, 2.035007953643799, 2.0709540843963623, 2.0973312854766846, 2.0984325408935547, 2.1067605018615723, 2.1400067806243896, 2.1226143836975098, 2.1394705772399902, 2.192946672439575, 2.1874918937683105, 2.193791389465332, 2.232042074203491, 2.2176766395568848, 2.234424114227295, 2.283104181289673, 2.3190455436706543, 2.3211262226104736, 2.328664779663086, 2.358736276626587, 2.376680374145508, 2.4619264602661133, 2.4509241580963135, 2.445204973220825, 2.4553346633911133, 2.4446327686309814, 2.489499807357788, 2.50454044342041, 2.545462131500244, 2.5696535110473633, 2.5860280990600586, 2.6310503482818604, 2.61860728263855, 2.650120496749878, 2.6408989429473877, 2.712642192840576, 2.7359578609466553, 2.7336080074310303, 2.7596211433410645, 2.7637627124786377, 2.8085718154907227, 2.756451368331909, 2.8046505451202393, 2.832426071166992, 2.861694812774658, 2.9084038734436035, 2.8765087127685547, 2.9351418018341064, 2.9929933547973633, 2.9980294704437256, 2.9754226207733154, 3.093231439590454, 3.1024279594421387, 3.0665829181671143, 3.1707379817962646, 3.1348984241485596, 3.1798882484436035, 3.2204508781433105, 3.185253143310547, 3.207838535308838, 3.2891483306884766, 3.3158762454986572, 3.2750415802001953, 3.347435235977173, 3.3806092739105225, 3.419053316116333, 3.3618226051330566, 3.4177207946777344, 3.421523332595825, 3.570127248764038, 3.4665822982788086, 3.481290102005005, 3.590559482574463, 3.5446653366088867, 3.6366684436798096, 3.5832433700561523, 3.6263046264648438, 3.6452465057373047, 3.653923749923706, 3.6898162364959717, 3.779819965362549, 3.770958662033081, 3.801238775253296, 3.779039144515991, 3.8491804599761963, 3.8273470401763916, 3.917947769165039, 3.9137470722198486, 3.9307727813720703, 3.9297072887420654, 3.9760239124298096, 4.004909515380859, 4.0503363609313965, 4.019541263580322, 4.021827220916748, 4.114411354064941, 4.111095905303955, 4.113736152648926, 4.196323871612549, 4.14992094039917, 4.215178489685059, 4.242473125457764, 4.225338459014893, 4.281238079071045, 4.286048412322998, 4.307961463928223, 4.348959445953369, 4.399361610412598, 4.368086338043213, 4.420629501342773, 4.3696675300598145, 4.4280524253845215, 4.437129020690918, 4.473901271820068, 4.587887287139893, 4.502207279205322, 4.530996322631836, 4.591538429260254, 4.602723121643066, 4.570942401885986, 4.6601762771606445, 4.677314281463623, 4.680015563964844, 4.667843818664551, 4.690303802490234, 4.777953147888184, 4.825526714324951, 4.80623722076416, 4.836915969848633, 4.804808616638184, 4.858688831329346, 5.074614524841309], 'val_accuracy': [0.38989999890327454, 0.4260999858379364, 0.46639999747276306, 0.4830000102519989, 0.5084999799728394, 0.51419997215271, 0.5320000052452087, 0.5515000224113464, 0.553600013256073, 0.5461000204086304, 0.5705000162124634, 0.5756999850273132, 0.5830000042915344, 0.5756000280380249, 0.5720999836921692, 0.5943999886512756, 0.5849000215530396, 0.6050999760627747, 0.6097000241279602, 0.6121000051498413, 0.5781999826431274, 0.6133000254631042, 0.6039999723434448, 0.6323999762535095, 0.6308000087738037, 0.6266999840736389, 0.6345000267028809, 0.6255999803543091, 0.6258999705314636, 0.6326000094413757, 0.6396999955177307, 0.6287000179290771, 0.6398000121116638, 0.6481000185012817, 0.6438999772071838, 0.6499999761581421, 0.6473000049591064, 0.6527000069618225, 0.6482999920845032, 0.6462000012397766, 0.6536999940872192, 0.6388999819755554, 0.6535999774932861, 0.6570000052452087, 0.6597999930381775, 0.6523000001907349, 0.6553000211715698, 0.6503999829292297, 0.6665999889373779, 0.6542999744415283, 0.6658999919891357, 0.6632999777793884, 0.6571000218391418, 0.6581000089645386, 0.6651999950408936, 0.6640999913215637, 0.6521999835968018, 0.6536999940872192, 0.659600019454956, 0.6636000275611877, 0.6563000082969666, 0.6636000275611877, 0.6650999784469604, 0.6643000245094299, 0.6542999744415283, 0.6692000031471252, 0.6583999991416931, 0.6661999821662903, 0.6571000218391418, 0.670799970626831, 0.6676999926567078, 0.6603000164031982, 0.6617000102996826, 0.6654000282287598, 0.6621999740600586, 0.6638000011444092, 0.6589999794960022, 0.6606000065803528, 0.6635000109672546, 0.6625000238418579, 0.6614000201225281, 0.6585000157356262, 0.6650000214576721, 0.6615999937057495, 0.6672000288963318, 0.6614000201225281, 0.6650999784469604, 0.6556000113487244, 0.6622999906539917, 0.6646000146865845, 0.6575999855995178, 0.663100004196167, 0.6549999713897705, 0.6608999967575073, 0.6625999808311462, 0.660099983215332, 0.6629999876022339, 0.6577000021934509, 0.6557999849319458, 0.6600000262260437, 0.6554999947547913, 0.6531999707221985, 0.6636000275611877, 0.6611999869346619, 0.6589999794960022, 0.659500002861023, 0.6614000201225281, 0.6589999794960022, 0.6581000089645386, 0.6636999845504761, 0.6554999947547913, 0.6561999917030334, 0.6597999930381775, 0.6610000133514404, 0.6571000218391418, 0.6560999751091003, 0.6496999859809875, 0.660099983215332, 0.656000018119812, 0.6539999842643738, 0.6583999991416931, 0.661899983882904, 0.6470999717712402, 0.656499981880188, 0.6531999707221985, 0.6550999879837036, 0.652899980545044, 0.6538000106811523, 0.6553000211715698, 0.6589000225067139, 0.6525999903678894, 0.6549000144004822, 0.6554999947547913, 0.6531000137329102, 0.6532999873161316, 0.6498000025749207, 0.6510000228881836, 0.6513000130653381, 0.6514000296592712, 0.6536999940872192, 0.6478000283241272, 0.6535000205039978, 0.6516000032424927, 0.6482999920845032, 0.6471999883651733, 0.6484000086784363, 0.6539000272750854, 0.65420001745224, 0.6510000228881836, 0.6545000076293945, 0.6524999737739563, 0.6480000019073486, 0.6492000222206116, 0.6419000029563904, 0.6507999897003174, 0.6439999938011169, 0.6453999876976013, 0.6473000049591064, 0.6464999914169312, 0.6468999981880188, 0.6484000086784363, 0.6462000012397766, 0.6438999772071838, 0.6468999981880188, 0.642799973487854, 0.6399000287055969, 0.6406999826431274, 0.6417999863624573, 0.6480000019073486, 0.637499988079071, 0.6474000215530396, 0.6395999789237976, 0.647599995136261, 0.6474999785423279, 0.6385999917984009, 0.6432999968528748, 0.644599974155426, 0.6437000036239624, 0.6446999907493591, 0.6425999999046326, 0.644599974155426, 0.6442999839782715, 0.6416000127792358, 0.642300009727478, 0.6410999894142151, 0.6366999745368958, 0.6399999856948853, 0.6414999961853027, 0.6420999765396118, 0.6345000267028809, 0.6445000171661377, 0.6448000073432922, 0.6359999775886536, 0.6410999894142151, 0.6396999955177307, 0.64410001039505, 0.6413000226020813, 0.6341999769210815, 0.6358000040054321, 0.6370000243186951, 0.6376000046730042, 0.6358000040054321, 0.6401000022888184, 0.635200023651123, 0.6384999752044678, 0.6381999850273132, 0.638700008392334, 0.6417999863624573, 0.637499988079071, 0.6376000046730042, 0.635699987411499, 0.6414999961853027, 0.6381999850273132, 0.6396999955177307, 0.6381999850273132, 0.6373000144958496, 0.6370000243186951, 0.6385999917984009, 0.6352999806404114, 0.6351000070571899, 0.6365000009536743, 0.6365000009536743, 0.6335999965667725, 0.6389999985694885, 0.638700008392334, 0.6366999745368958, 0.6384999752044678, 0.635699987411499, 0.6331999897956848, 0.6359000205993652, 0.637499988079071, 0.6383000016212463, 0.6304000020027161, 0.6327000260353088, 0.6388999819755554, 0.6348999738693237, 0.6373999714851379, 0.6295999884605408, 0.6352999806404114, 0.6380000114440918, 0.6335999965667725, 0.6377999782562256, 0.6359000205993652, 0.6337000131607056, 0.6373000144958496, 0.6335999965667725, 0.6378999948501587, 0.6360999941825867, 0.6371999979019165, 0.6366000175476074, 0.6341000199317932, 0.6349999904632568, 0.6334999799728394, 0.6328999996185303, 0.6366999745368958, 0.6308000087738037, 0.6331999897956848, 0.6313999891281128, 0.6323999762535095, 0.6381999850273132, 0.6337000131607056, 0.6366999745368958, 0.6323000192642212, 0.6342999935150146, 0.6355999708175659, 0.6351000070571899, 0.6330999732017517, 0.6377999782562256, 0.6360999941825867, 0.6345000267028809, 0.6370000243186951, 0.6323999762535095, 0.6327999830245972, 0.637499988079071, 0.6348999738693237, 0.6373999714851379, 0.6395000219345093, 0.6385999917984009, 0.6359999775886536, 0.6359000205993652, 0.6381000280380249, 0.6370999813079834, 0.6330999732017517, 0.6381000280380249, 0.6373999714851379, 0.6373000144958496, 0.6403999924659729, 0.633899986743927, 0.6317999958992004, 0.6337000131607056, 0.6371999979019165, 0.6370000243186951, 0.6370000243186951, 0.6327000260353088, 0.6338000297546387, 0.6381999850273132, 0.6371999979019165, 0.6363999843597412, 0.6370000243186951, 0.628000020980835]}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# initialization of optimizer \n",
    "opt = keras.optimizers.RMSprop(learning_rate = 0.0001,decay = 1e-6)\n",
    "\n",
    "# train the model by optimizer\n",
    "cnn_model.compile(loss = 'categorical_crossentropy',\n",
    "              optimizer = opt,\n",
    "              metrics = ['accuracy'])\n",
    "\n",
    "train_set = train_set.astype('float32')\n",
    "test_set = test_set.astype('float32') \n",
    "\n",
    "train_set /= 255 \n",
    "test_set /= 255 \n",
    "\n",
    "if not is_data_augmentation:\n",
    "    print('Not using data augmentation.')\n",
    "    cnn_history = cnn_model.fit(train_set, train_label,\n",
    "              batch_size=batch_size,\n",
    "              epochs=num_epochs,\n",
    "              validation_data=(test_set, test_label),\n",
    "              shuffle=True)\n",
    "else:\n",
    "    print('Using real-time data augmentation.')\n",
    "    # This will do preprocessing and realtime data augmentation:\n",
    "    augument_data_set = ImageDataGenerator(\n",
    "        featurewise_center=False,             # set input mean to 0 over the dataset\n",
    "        samplewise_center=False,              # set each sample mean to 0\n",
    "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "        samplewise_std_normalization=False,   # divide each input by its std\n",
    "        zca_whitening=False,                  # apply ZCA whitening\n",
    "        zca_epsilon=1e-06,                    # epsilon for ZCA whitening\n",
    "        rotation_range=0,                     # randomly rotate images in the range (degrees, 0 to 180)\n",
    "        \n",
    "        # randomly shift images horizontally (fraction of total width)\n",
    "        width_shift_range=0.1,\n",
    "        # randomly shift images vertically (fraction of total height)\n",
    "        height_shift_range=0.1,\n",
    "        \n",
    "        shear_range=0.,  # set range for random shear\n",
    "        zoom_range=0.,  # set range for random zoom\n",
    "        channel_shift_range=0.,  # set range for random channel shifts\n",
    "        # set mode for filling points outside the input boundaries\n",
    "        fill_mode='nearest',\n",
    "        cval=0.,  # value used for fill_mode = \"constant\"\n",
    "        horizontal_flip=True,  # randomly flip images\n",
    "        vertical_flip=False,  # randomly flip images\n",
    "        # set rescaling factor (applied before any other transformation)\n",
    "        rescale=None,\n",
    "        # set function that will be applied on each input\n",
    "        preprocessing_function=None,\n",
    "        # image data format, either \"channels_first\" or \"channels_last\"\n",
    "        data_format=None,\n",
    "        # fraction of images reserved for validation (strictly between 0 and 1)\n",
    "        validation_split=0.0)\n",
    "        \n",
    "    # compute quantities required for feature-wise normalization \n",
    "    # (std, mean ,and pricipal components if ZCA whitening is applied)\n",
    "    augument_data_set.fit(train_set) \n",
    "    \n",
    "    \n",
    "    # Fit the model on the batches generated by augument_data_set.flow() \n",
    "    \n",
    "    cnn_model.fit_generator(augument_data_set.flow(train_set,train_label ,\n",
    "                                                  batch_size=batch_size),\n",
    "                           epochs = num_epochs, \n",
    "                           validation_data = (test_set,test_label),\n",
    "                           workers = 4 )\n",
    "    \n",
    "# save model and weights \n",
    "if not os.path.isdir(model_dir): \n",
    "    os.makedirs(model_dir) \n",
    "model_path = os.path.join(model_dir,model_filename)\n",
    "cnn_model.save(model_path)\n",
    "print(\"CNN Model saved at %s \" % model_path)\n",
    "\n",
    "# Score trained model \n",
    "\n",
    "test_loss_value, test_metric_value = cnn_model.evaluate(test_set,test_label,verbose =1 )\n",
    "train_loss_value, train_metric_value = cnn_model.evaluate(train_set,train_label,verbose =1)\n",
    "\n",
    "\n",
    "print(\"Train loss: \", train_loss_value) \n",
    "print(\"Trian metric: \", train_metric_value)\n",
    "\n",
    "print(\"Test loss: \", test_loss_value)\n",
    "print(\"Test accuracy:\", test_metric_value)\n",
    "print(cnn_history.history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'test loss and test accuracy')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# get history data\n",
    "epoth_x = np.linspace(0, 10, num_epochs, endpoint=True)\n",
    "train_loss = cnn_history.history['loss']\n",
    "train_accuracy = cnn_history.history['accuracy']\n",
    "test_loss = cnn_history.history['val_loss']\n",
    "test_accuracy = cnn_history.history['val_accuracy']\n",
    "\n",
    "# plot train loss and accuracy\n",
    "plt.figure() \n",
    "plt.plot(epoth_x, train_loss,label = 'train loss')\n",
    "plt.plot(epoth_x,train_accuracy,label = 'train accuracy')\n",
    "plt.title(\"train loss and train accuracy\")\n",
    "plt.legend()\n",
    "\n",
    "\n",
    "# plot test loss and accuracy\n",
    "plt.figure() \n",
    "plt.plot(epoth_x, test_loss, label = 'test loss')\n",
    "plt.plot(epoth_x,test_accuracy, label = 'test accuracy')\n",
    "plt.legend()\n",
    "plt.title(\"test loss and test accuracy\")\n",
    "\n",
    "# get max test accuracy and \n",
    "\n",
    "print(\"The max test accuracy is \",max(test_accuracy))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
