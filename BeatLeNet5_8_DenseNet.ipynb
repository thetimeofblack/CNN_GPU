{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--gpu GPU] [--load LOAD] [--drop_1 DROP_1]\n",
      "                             [--drop_2 DROP_2] [--depth DEPTH]\n",
      "                             [--max_epoch MAX_EPOCH]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f C:\\Users\\12414\\AppData\\Roaming\\jupyter\\runtime\\kernel-477c7fbf-56a2-465d-b5f2-518179aa21dd.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\12414\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3334: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: UTF-8 -*-\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import argparse\n",
    "import os\n",
    "\n",
    "\n",
    "from tensorpack import *\n",
    "from tensorpack.tfutils.symbolic_functions import *\n",
    "from tensorpack.tfutils.summary import *\n",
    "\n",
    "\"\"\"\n",
    "CIFAR10 DenseNet example. See: http://arxiv.org/abs/1608.06993\n",
    "Code is developed based on Yuxin Wu's ResNet implementation: https://github.com/ppwwyyxx/tensorpack/tree/master/examples/ResNet\n",
    "Results using DenseNet (L=40, K=12) on Cifar10 with data augmentation: ~5.77% test error.\n",
    "\n",
    "Running time:\n",
    "On one TITAN X GPU (CUDA 7.5 and cudnn 5.1), the code should run ~5iters/s on a batch size 64.\n",
    "\"\"\"\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "class Model(ModelDesc):\n",
    "    def __init__(self, depth):\n",
    "        super(Model, self).__init__()\n",
    "        self.N = int((depth - 4)  / 3)\n",
    "        self.growthRate =12\n",
    "\n",
    "    def _get_inputs(self):\n",
    "        return [InputDesc(tf.float32, [None, 32, 32, 3], 'input'),\n",
    "                InputDesc(tf.int32, [None], 'label')\n",
    "               ]\n",
    "\n",
    "    def _build_graph(self, input_vars):\n",
    "        image, label = input_vars\n",
    "        image = image / 128.0 - 1\n",
    "\n",
    "        def conv(name, l, channel, stride):\n",
    "            return Conv2D(name, l, channel, 3, stride=stride,\n",
    "                          nl=tf.identity, use_bias=False,\n",
    "                          W_init=tf.random_normal_initializer(stddev=np.sqrt(2.0/9/channel)))\n",
    "        def add_layer(name, l):\n",
    "            shape = l.get_shape().as_list()\n",
    "            in_channel = shape[3]\n",
    "            with tf.variable_scope(name) as scope:\n",
    "                c = BatchNorm('bn1', l)\n",
    "                c = tf.nn.relu(c)\n",
    "                c = conv('conv1', c, self.growthRate, 1)\n",
    "                l = tf.concat([c, l], 3)\n",
    "            return l\n",
    "\n",
    "        def add_transition(name, l):\n",
    "            shape = l.get_shape().as_list()\n",
    "            in_channel = shape[3]\n",
    "            with tf.variable_scope(name) as scope:\n",
    "                l = BatchNorm('bn1', l)\n",
    "                l = tf.nn.relu(l)\n",
    "                l = Conv2D('conv1', l, in_channel, 1, stride=1, use_bias=False, nl=tf.nn.relu)\n",
    "                l = AvgPooling('pool', l, 2)\n",
    "            return l\n",
    "\n",
    "\n",
    "        def dense_net(name):\n",
    "            l = conv('conv0', image, 16, 1)\n",
    "            with tf.variable_scope('block1') as scope:\n",
    "\n",
    "                for i in range(self.N):\n",
    "                    l = add_layer('dense_layer.{}'.format(i), l)\n",
    "                l = add_transition('transition1', l)\n",
    "\n",
    "            with tf.variable_scope('block2') as scope:\n",
    "\n",
    "                for i in range(self.N):\n",
    "                    l = add_layer('dense_layer.{}'.format(i), l)\n",
    "                l = add_transition('transition2', l)\n",
    "\n",
    "            with tf.variable_scope('block3') as scope:\n",
    "\n",
    "                for i in range(self.N):\n",
    "                    l = add_layer('dense_layer.{}'.format(i), l)\n",
    "            l = BatchNorm('bnlast', l)\n",
    "            l = tf.nn.relu(l)\n",
    "            l = GlobalAvgPooling('gap', l)\n",
    "            logits = FullyConnected('linear', l, out_dim=10, nl=tf.identity)\n",
    "\n",
    "            return logits\n",
    "\n",
    "        logits = dense_net(\"dense_net\")\n",
    "\n",
    "        prob = tf.nn.softmax(logits, name='output')\n",
    "\n",
    "        cost = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=label)\n",
    "        cost = tf.reduce_mean(cost, name='cross_entropy_loss')\n",
    "\n",
    "        wrong = prediction_incorrect(logits, label)\n",
    "        # monitor training error\n",
    "        add_moving_summary(tf.reduce_mean(wrong, name='train_error'))\n",
    "\n",
    "        # weight decay on all W\n",
    "        wd_cost = tf.multiply(1e-4, regularize_cost('.*/W', tf.nn.l2_loss), name='wd_cost')\n",
    "        add_moving_summary(cost, wd_cost)\n",
    "\n",
    "        add_param_summary(('.*/W', ['histogram']))   # monitor W\n",
    "        self.cost = tf.add_n([cost, wd_cost], name='cost')\n",
    "\n",
    "    def _get_optimizer(self):\n",
    "        lr = tf.get_variable('learning_rate', initializer=0.1, trainable=False)\n",
    "        tf.summary.scalar('learning_rate', lr)\n",
    "        return tf.train.MomentumOptimizer(lr, 0.9, use_nesterov=True)\n",
    "\n",
    "\n",
    "def get_data(train_or_test):\n",
    "    isTrain = train_or_test == 'train'\n",
    "    ds = dataset.Cifar10(train_or_test)\n",
    "    pp_mean = ds.get_per_pixel_mean()\n",
    "    if isTrain:\n",
    "        augmentors = [\n",
    "            imgaug.CenterPaste((40, 40)),\n",
    "            imgaug.RandomCrop((32, 32)),\n",
    "            imgaug.Flip(horiz=True),\n",
    "            #imgaug.Brightness(20),\n",
    "            #imgaug.Contrast((0.6,1.4)),\n",
    "            imgaug.MapImage(lambda x: x - pp_mean),\n",
    "        ]\n",
    "    else:\n",
    "        augmentors = [\n",
    "            imgaug.MapImage(lambda x: x - pp_mean)\n",
    "        ]\n",
    "    ds = AugmentImageComponent(ds, augmentors)\n",
    "    ds = BatchData(ds, BATCH_SIZE, remainder=not isTrain)\n",
    "    if isTrain:\n",
    "        ds = PrefetchData(ds, 3, 2)\n",
    "    return ds\n",
    "\n",
    "def get_config():\n",
    "    log_dir = 'train_log/cifar10-single-fisrt%s-second%s-max%s' % (str(args.drop_1), str(args.drop_2), str(args.max_epoch))\n",
    "    logger.set_logger_dir(log_dir, action='n')\n",
    "\n",
    "    # prepare dataset\n",
    "    dataset_train = get_data('train')\n",
    "    steps_per_epoch = dataset_train.size()\n",
    "    dataset_test = get_data('test')\n",
    "\n",
    "    return TrainConfig(\n",
    "        dataflow=dataset_train,\n",
    "        callbacks=[\n",
    "            ModelSaver(),\n",
    "            InferenceRunner(dataset_test,\n",
    "                [ScalarStats('cost'), ClassificationError()]),\n",
    "            ScheduledHyperParamSetter('learning_rate',\n",
    "                                      [(1, 0.1), (args.drop_1, 0.01), (args.drop_2, 0.001)])\n",
    "        ],\n",
    "        model=Model(depth=args.depth),\n",
    "        steps_per_epoch=steps_per_epoch,\n",
    "        max_epoch=args.max_epoch,\n",
    "    )\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--gpu', help='comma separated list of GPU(s) to use.') # nargs='*' in multi mode\n",
    "    parser.add_argument('--load', help='load model')\n",
    "    parser.add_argument('--drop_1',default=150, help='Epoch to drop learning rate to 0.01.') # nargs='*' in multi mode\n",
    "    parser.add_argument('--drop_2',default=225,help='Epoch to drop learning rate to 0.001')\n",
    "    parser.add_argument('--depth',default=40, help='The depth of densenet')\n",
    "    parser.add_argument('--max_epoch',default=300,help='max epoch')\n",
    "    args = parser.parse_args()\n",
    "    args.gpu = 1\n",
    "    args.load = False \n",
    "    \n",
    "    if args.gpu:\n",
    "        os.environ['CUDA_VISIBLE_DEVICES'] = args.gpu\n",
    "\n",
    "    config = get_config()\n",
    "    if args.load:\n",
    "        config.session_init = SaverRestore(args.load)\n",
    "    \n",
    "    nr_tower = 0\n",
    "    if args.gpu:\n",
    "        nr_tower = len(args.gpu.split(','))\n",
    "    \n",
    "    # SyncMultiGPUTrainer(config).train()\n",
    "    launch_train_with_config(config, SyncMultiGPUTrainer(nr_tower))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
