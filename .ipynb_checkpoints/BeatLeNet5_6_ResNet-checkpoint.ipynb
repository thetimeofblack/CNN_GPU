{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "# import some keras and basic module \n",
    "\n",
    "from __future__ import print_function \n",
    "import tensorflow.keras as keras\n",
    "\n",
    "# data set cifar 10 \n",
    "from tensorflow.keras.datasets import cifar10 \n",
    "\n",
    "# basic preprocesssing for image data \n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# construct neural network \n",
    "from tensorflow.keras.models import Sequential \n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten \n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras import layers \n",
    "from tensorflow.keras import models \n",
    "import os \n",
    "\n",
    "import numpy as np \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.keras.utils import plot_model\n",
    "import tensorflow as tf \n",
    "\n",
    "config = tf.ConfigProto()\n",
    "tf.enable_eager_execution(config=config)\n",
    "\n",
    "from fmp import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## basic parameters \n",
    "batch_size = 40 \n",
    "num_classes = 10 \n",
    "num_epochs = 300 \n",
    "is_data_augmentation = True \n",
    "model_dir = 'models'\n",
    "model_filename = 'model_BeatLeNet5_6_ResNet.h'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method Network.summary of <tensorflow.python.keras.engine.training.Model object at 0x000001D6A1B2F848>>\n"
     ]
    }
   ],
   "source": [
    "def residual_network(x):\n",
    "    \n",
    "    def add_common_layers(y):\n",
    "        y = layers.BatchNormalization()(y)\n",
    "        y = layers.LeakyReLU()(y)\n",
    "        \n",
    "        return y \n",
    "    def grouped_convolution(y,nb_channels,_strides):\n",
    "        if cardinality == 1:\n",
    "            return layers.Conv2D(nb_channels, kernel_size=(3,3), strides=_strides, padding='same')(y)\n",
    "        assert not nb_channels % cardinality \n",
    "        _d = nb_channels // cardinality \n",
    "        \n",
    "        \n",
    "        groups = []\n",
    "        for j in range(cardinality):\n",
    "            group = layers.Lambda(lambda z: z[:, :, :, j* _d:j * _d + _d])(y)\n",
    "            groups.append(layers.Conv2D(_d,kernel_size=(3,3),strides = _strides, padding = 'same')(group))\n",
    "            \n",
    "        y = layers.concatenate(groups)\n",
    "        \n",
    "        return y \n",
    "    \n",
    "    def residual_block(y, nb_channels_in,nb_channels_out,_strides = (1,1), _project_shortcut= False):\n",
    "        \"\"\"\n",
    "        Our network consists of a stack of residual blocks. These blocks have the same topology, \n",
    "        and are subject to two simple rules:\n",
    "        \n",
    "        - If producing spatial maps of the same size, the blocks share the same \n",
    "        \n",
    "        \"\"\"\n",
    "        shortcut = y \n",
    "        \n",
    "        y = layers.Conv2D(nb_channels_in, kernel_size=(1,1) , strides=(1,1) , padding='same')(y)\n",
    "        y = add_common_layers(y)\n",
    "        \n",
    "        y = grouped_convolution(y,nb_channels_in,_strides=_strides)\n",
    "        y = add_common_layers(y)\n",
    "        \n",
    "        y = layers.Conv2D(nb_channels_out,kernel_size=(1,1),strides=(1,1), padding='same')(y)\n",
    "        \n",
    "        y = layers.BatchNormalization()(y)\n",
    "        \n",
    "        if _project_shortcut or _strides != (1, 1):\n",
    "            \n",
    "            shortcut = layers.Conv2D(nb_channels_out, kernel_size=(1,1), strides=_strides, padding='same')(shortcut)\n",
    "            shortcut = layers.BatchNormalization()(shortcut)\n",
    "            \n",
    "        y = layers.add([shortcut, y])\n",
    "        \n",
    "        y = layers.LeakyReLU()(y)\n",
    "        \n",
    "        return y \n",
    "    \n",
    "    x = layers.Conv2D(64,kernel_size=(7,7), strides=(2,2), padding='same')(x)\n",
    "    x = add_common_layers(x)\n",
    "    \n",
    "    x = layers.MaxPool2D(pool_size=(3,3), strides=(2,2), padding='same')(x)\n",
    "    for i in range(3):\n",
    "        project_shortcut = True if i == 0 else False \n",
    "        x = residual_block(x,128,256,_project_shortcut=project_shortcut)\n",
    "        \n",
    "        \n",
    "    for i in range(4):\n",
    "        strides = (2,2) if i == 0 else (1,1)\n",
    "        x = residual_block(x, 256, 512, _strides=strides)\n",
    "        \n",
    "    for i in range(6):\n",
    "        strides = (2,2) if i==0 else (1,1)\n",
    "        x = residual_block(x, 1024, 2048, _strides=strides)\n",
    "    \n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    x = layers.Dense(10)(x)\n",
    "    \n",
    "    return x \n",
    "img_height = 32\n",
    "img_width = 32 \n",
    "img_channels = 3 \n",
    "cardinality = 32\n",
    "image_tensor = layers.Input(shape=(img_height, img_width, img_channels))\n",
    "network_output = residual_network(image_tensor)\n",
    "\n",
    "cnn_model = models.Model(inputs=[image_tensor], outputs=[network_output])\n",
    "\n",
    "print(cnn_model.summary)\n",
    "#plot_model(cnn_model, show_shapes=True,to_file='cnn_model.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 10)\n",
      "(10000, 10)\n",
      "the shape of training data set is:  (50000, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "#read data and split data into train and test set\n",
    "\n",
    "(train_set,train_label),(test_set,test_label) = cifar10.load_data() \n",
    "\n",
    "# Convert class label to binary vector \n",
    "train_label = keras.utils.to_categorical(train_label,num_classes)\n",
    "test_label  = keras.utils.to_categorical(test_label,num_classes) \n",
    "print(train_label.shape)\n",
    "print(test_label.shape)\n",
    "print('the shape of training data set is: ',train_set.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using real-time data augmentation.\n",
      "(50000, 32, 32, 3)\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Epoch 1/300\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# initialization of optimizer \n",
    "opt = keras.optimizers.RMSprop(learning_rate=0.001, rho=0.9)\n",
    "\n",
    "# train the model by optimizer\n",
    "cnn_model.compile(loss = 'categorical_crossentropy',\n",
    "              optimizer = opt,\n",
    "              metrics = ['accuracy'])\n",
    "\n",
    "train_set = train_set.astype('float32')\n",
    "test_set = test_set.astype('float32') \n",
    "\n",
    "train_set /= 255 \n",
    "test_set /= 255 \n",
    "\n",
    "if not is_data_augmentation:\n",
    "    print('Not using data augmentation.')\n",
    "    cnn_history = cnn_model.fit(train_set, train_label,\n",
    "              batch_size=batch_size,\n",
    "              epochs=num_epochs,\n",
    "              validation_data=(test_set, test_label),\n",
    "              shuffle=True)\n",
    "else:\n",
    "    print('Using real-time data augmentation.')\n",
    "    # This will do preprocessing and realtime data augmentation:\n",
    "     \n",
    "    \n",
    "    # set parameter for data augmentation \n",
    "    data_transform_parameters = {\n",
    "                                \"rotation_range\": 15,\n",
    "                                \"width_shift_range\":0.1,\n",
    "                                \"height_shift_range\":0.1,\n",
    "                                \"horizontal_flip\":True\n",
    "                                }\n",
    "    \n",
    "    # get object of augmentation data generator \n",
    "    augment_data_set_generator = ImageDataGenerator(data_transform_parameters)    \n",
    "    \n",
    "    \n",
    "    # get augmented data set \n",
    "    \n",
    "    augment_train_set = augment_data_set_generator.apply_transform(train_set,data_transform_parameters)\n",
    "    print(augment_train_set.shape)\n",
    "  \n",
    "    \n",
    "    # Limit GPU device to the first GPU \n",
    "    #gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "    #if gpus:\n",
    "      # Restrict TensorFlow to only use the first GPU\n",
    "    #  try:\n",
    "    #    tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n",
    "    #    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "    #    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPU\")\n",
    "    #  except RuntimeError as e:\n",
    "        # Visible devices must be set before GPUs have been initialized\n",
    "    #    print(e)\n",
    "        \n",
    "        \n",
    "    # train model by GPU \n",
    "    with tf.device('/device:GPU:0'):\n",
    "        cnn_history = cnn_model.fit(augment_train_set, train_label,\n",
    "                  batch_size=batch_size,\n",
    "                  epochs=num_epochs,\n",
    "                  validation_data=(test_set, test_label),\n",
    "                  shuffle=True)\n",
    "\n",
    "# save model and weights \n",
    "if not os.path.isdir(model_dir): \n",
    "    os.makedirs(model_dir) \n",
    "model_path = os.path.join(model_dir,model_filename)\n",
    "cnn_model.save(model_path)\n",
    "print(\"CNN Model saved at %s \" % model_path)\n",
    "\n",
    "# Score trained model \n",
    "\n",
    "test_loss_value, test_metric_value = cnn_model.evaluate(test_set,test_label,verbose =1 )\n",
    "train_loss_value, train_metric_value = cnn_model.evaluate(train_set,train_label,verbose =1)\n",
    "\n",
    "\n",
    "print(\"Train loss: \", train_loss_value) \n",
    "print(\"Trian metric: \", train_metric_value)\n",
    "\n",
    "print(\"Test loss: \", test_loss_value)\n",
    "print(\"Test accuracy:\", test_metric_value)\n",
    "# print(cnn_history.history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# get history data\n",
    "epoth_x = np.linspace(0, 10, num_epochs, endpoint=True)\n",
    "train_loss = cnn_history.history['loss']\n",
    "train_accuracy = cnn_history.history['acc']\n",
    "test_loss = cnn_history.history['val_loss']\n",
    "test_accuracy = cnn_history.history['val_acc']\n",
    "\n",
    "# plot train loss and accuracy\n",
    "plt.figure() \n",
    "plt.plot(epoth_x, train_loss,label = 'train loss')\n",
    "plt.plot(epoth_x,train_accuracy,label = 'train accuracy')\n",
    "plt.title(\"train loss and train accuracy\")\n",
    "plt.legend()\n",
    "\n",
    "\n",
    "# plot test loss and accuracy\n",
    "plt.figure() \n",
    "plt.plot(epoth_x, test_loss, label = 'test loss')\n",
    "plt.plot(epoth_x,test_accuracy, label = 'test accuracy')\n",
    "plt.legend()\n",
    "plt.title(\"test loss and test accuracy\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
