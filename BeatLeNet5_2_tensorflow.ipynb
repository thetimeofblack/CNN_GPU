{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow' has no attribute 'ConfigProto'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-57d9c4f862e5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m \u001b[0mconfig\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mConfigProto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menable_eager_execution\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'ConfigProto'"
     ]
    }
   ],
   "source": [
    "# import some keras and basic module \n",
    "\n",
    "from __future__ import print_function \n",
    "import tensorflow.keras as keras\n",
    "\n",
    "# data set cifar 10 \n",
    "from tensorflow.keras.datasets import cifar10 \n",
    "\n",
    "# basic preprocesssing for image data \n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# construct neural network \n",
    "from tensorflow.keras.models import Sequential \n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten \n",
    "\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "\n",
    "import os \n",
    "\n",
    "import numpy as np \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.keras.utils import plot_model\n",
    "import tensorflow as tf \n",
    "\n",
    "# config = tf.ConfigProto()\n",
    "# tf.enable_eager_execution(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## basic parameters \n",
    "batch_size = 32 \n",
    "num_classes = 10 \n",
    "num_epochs = 300 \n",
    "is_data_augmentation = True \n",
    "model_dir = 'models'\n",
    "model_filename = 'BeatLeNet5_2_FMP.h'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read data and split data into train and test set\n",
    "\n",
    "(train_set,train_label),(test_set,test_label) = cifar10.load_data() \n",
    "\n",
    "print('the shape of training data set is: ',train_set.shape) \n",
    "\n",
    "# print number of train and test samples \n",
    "\n",
    "# train samples 50000\n",
    "#print(train_set.shape[0] , 'train samples') \n",
    "\n",
    "# test samples 10000\n",
    "#print(test_set.shape[0] , 'test samples') \n",
    "\n",
    "#print(train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert class label to binary vector \n",
    "train_label = keras.utils.to_categorical(train_label,num_classes)\n",
    "test_label  = keras.utils.to_categorical(test_label,num_classes) \n",
    "print(train_label.shape)\n",
    "print(test_label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build Max Fractional Pooling Network\n",
    "def initialize_mfp_layer(X):\n",
    "    mean = 0\n",
    "    sigma = 2 \n",
    "    conv_w = tf.Variable(tf.truncated_normal(shape = [2,2,1,160],mean = mean, stddev = sigma))\n",
    "    conv_b = tf.Variable(tf.zeros(160))\n",
    "    conv = tf.nn.conv2d(X,conv_w,strides = [1,1,1,1] ,padding = 'same')\n",
    "    conv = tf.nn.relu(conv)\n",
    "    pool = tf.nn.fractional_max_pool(conv,ksize = [1,2,2,1], strides = [1,2,2,1],padding ='same')\n",
    "    return pool\n",
    "def generate_mfp_next_layer(previous_layer, n):\n",
    "    mean = 0\n",
    "    sigma = 2 \n",
    "    conv_w = tf.Variable(tf.truncated_normal(shape = [2,2,1,n*160],mean = mean, stddev = sigma))\n",
    "    conv_b = tf.Variable(tf.zeros(n*160))\n",
    "    conv = tf.nn.conv2d(previous_layer,conv_w,strides = [1,1,1,1] ,padding = 'same')\n",
    "    conv = tf.nn.relu(conv)\n",
    "    pool = tf.nn.fractional_max_pool(conv,ksize = [1,2,2,1], strides = [1,2,2,1],padding ='same')\n",
    "    return pool\n",
    "def mfp_Net(X,n):\n",
    "      \n",
    "    intialized_layer = initialize_mfp_layer(X)\n",
    "    previous_layer = initialized_layer \n",
    "    for n = 2:12:\n",
    "        previous_layer =generate_mfp_next_layer(previous_layer,n)\n",
    "        \n",
    "    \n",
    "    return previous_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, (None, 32, 32, 1))\n",
    "y = tf.placeholder(tf.int32, (None))\n",
    "one_hot_y = tf.one_hot(y, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rate = 0.001\n",
    "\n",
    "logits = mfp_Net(x)\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits = logits, labels = one_hot_y)\n",
    "loss_operation = tf.reduce_mean(cross_entropy)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate = rate)\n",
    "training_operation = optimizer.minimize(loss_operation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(one_hot_y, 1))\n",
    "accuracy_operation = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the code input for Conv2D \n",
    "```\n",
    "tf.keras.layers.Conv2D(\n",
    "    filters, kernel_size, strides=(1, 1), padding='valid', data_format=None,\n",
    "    dilation_rate=(1, 1), activation=None, use_bias=True,\n",
    "    kernel_initializer='glorot_uniform', bias_initializer='zeros',\n",
    "    kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None,\n",
    "    kernel_constraint=None, bias_constraint=None, **kwargs\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# initialization of optimizer \n",
    "opt = keras.optimizers.SGD(learning_rate = 0.0001)\n",
    "\n",
    "# train the model by optimizer\n",
    "cnn_model.compile(loss = 'categorical_crossentropy',\n",
    "              optimizer = opt,\n",
    "              metrics = ['accuracy'])\n",
    "\n",
    "train_set = train_set.astype('float32')\n",
    "test_set = test_set.astype('float32') \n",
    "\n",
    "train_set /= 255 \n",
    "test_set /= 255 \n",
    "\n",
    "if not is_data_augmentation:\n",
    "    print('Not using data augmentation.')\n",
    "    cnn_history = cnn_model.fit(train_set, train_label,\n",
    "              batch_size=batch_size,\n",
    "              epochs=num_epochs,\n",
    "              validation_data=(test_set, test_label),\n",
    "              shuffle=True)\n",
    "else:\n",
    "    print('Using real-time data augmentation.')\n",
    "    # This will do preprocessing and realtime data augmentation:\n",
    "     \n",
    "    \n",
    "    # set parameter for data augmentation \n",
    "    data_transform_parameters = {\n",
    "                                \"featurewise_center\":True,\n",
    "                                \"featurewise_std_normalization\" : True,\n",
    "                                \"rotation_range\": 20,\n",
    "                                \"width_shift_range\":0.2,\n",
    "                                \"height_shift_range\":0.2,\n",
    "                                \"horizontal_flip\":True\n",
    "                                }\n",
    "    \n",
    "    # get object of augmentation data generator \n",
    "    augment_data_set_generator = ImageDataGenerator(data_transform_parameters)    \n",
    "    \n",
    "    \n",
    "    # get augmented data set \n",
    "    \n",
    "    augment_train_set = augment_data_set_generator.apply_transform(train_set,data_transform_parameters)\n",
    "    print(augment_train_set.shape)\n",
    "  \n",
    "    \n",
    "    # Limit GPU device to the first GPU \n",
    "    gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "    if gpus:\n",
    "      # Restrict TensorFlow to only use the first GPU\n",
    "      try:\n",
    "        tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPU\")\n",
    "      except RuntimeError as e:\n",
    "        # Visible devices must be set before GPUs have been initialized\n",
    "        print(e)\n",
    "        \n",
    "        \n",
    "    # train model by GPU \n",
    "    with tf.device('/device:GPU:0'):\n",
    "        cnn_history = cnn_model.fit(augument_train_set, train_label,\n",
    "                  batch_size=batch_size,\n",
    "                  epochs=num_epochs,\n",
    "                  validation_data=(test_set, test_label),\n",
    "                  shuffle=True)\n",
    "    \n",
    "# save model and weights \n",
    "if not os.path.isdir(model_dir): \n",
    "    os.makedirs(model_dir) \n",
    "model_path = os.path.join(model_dir,model_filename)\n",
    "cnn_model.save(model_path)\n",
    "print(\"CNN Model saved at %s \" % model_path)\n",
    "\n",
    "# Score trained model \n",
    "\n",
    "test_loss_value, test_metric_value = cnn_model.evaluate(test_set,test_label,verbose =1 )\n",
    "train_loss_value, train_metric_value = cnn_model.evaluate(train_set,train_label,verbose =1)\n",
    "\n",
    "\n",
    "print(\"Train loss: \", train_loss_value) \n",
    "print(\"Trian metric: \", train_metric_value)\n",
    "\n",
    "print(\"Test loss: \", test_loss_value)\n",
    "print(\"Test accuracy:\", test_metric_value)\n",
    "# print(cnn_history.history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# get history data\n",
    "epoth_x = np.linspace(0, 10, num_epochs, endpoint=True)\n",
    "train_loss = cnn_history.history['loss']\n",
    "train_accuracy = cnn_history.history['accuracy']\n",
    "test_loss = cnn_history.history['val_loss']\n",
    "test_accuracy = cnn_history.history['val_accuracy']\n",
    "\n",
    "# plot train loss and accuracy\n",
    "plt.figure() \n",
    "plt.plot(epoth_x, train_loss,label = 'train loss')\n",
    "plt.plot(epoth_x,train_accuracy,label = 'train accuracy')\n",
    "plt.title(\"train loss and train accuracy\")\n",
    "plt.legend()\n",
    "\n",
    "\n",
    "# plot test loss and accuracy\n",
    "plt.figure() \n",
    "plt.plot(epoth_x, test_loss, label = 'test loss')\n",
    "plt.plot(epoth_x,test_accuracy, label = 'test accuracy')\n",
    "plt.legend()\n",
    "plt.title(\"test loss and test accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
